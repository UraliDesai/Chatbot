{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled_augmen(_liniear,drop_,Lr,atte).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-IjaOq6M_fY3"
      },
      "source": [
        "__Flowchart__\n",
        "\n",
        "The following flowchart shows roughly how the neural network is constructed. It is split into two parts: An encoder which maps the source-text to a \"thought vector\" that summarizes the text's contents, which is then input to the second part of the neural network that decodes the \"thought vector\" to the response-text.\n",
        "\n",
        "The neural network cannot work directly on text so first we need to convert each word to an integer-token using a tokenizer. But the neural network cannot work on integers either, so we use a so-called Embedding Layer to convert each integer-token to a vector of floating-point values. The embedding is trained alongside the rest of the neural network to map words with similar semantic meaning to similar vectors of floating-point values.\n",
        "\n",
        "For example, consider the input text is \"how are you?\" and excpected response in text is \"I am fine thanks for asking me\". We first convert the entire data-set to integer-tokens so the text \"how are you?\" becomes [6, 1, 2]. Each of these integer-tokens is then mapped to an embedding-vector with e.g.8 elements, so the integer-token 6 could for example become [0.12, -0.56, ..., 1.19] and the integer-token 1 could for example become [0.39, 0.09, ..., -0.12]. These embedding-vectors can then be input to the Recurrent Neural Network, which has 3 GRU-layers.\n",
        "\n",
        "The last GRU-layer outputs a single vector - the \"thought vector\" that summarizes the contents of the source-text - which is then used as the initial state of the GRU-units in the decoder-part.\n",
        "\n",
        "The destination-text \"I am fine thanks for asking me\" is padded with special markers \"ssss\" and \"eeee\" to indicate its beginning and end, so the sequence of integer-tokens becomes [1,6,7,8,9,10,11,12,2]. During training, the decoder will be given this entire sequence as input and the desired output sequence is [6,7, 8, 9,10,11,12,2] which is the same sequence but time-shifted one step. We are trying to teach the decoder to map the \"thought vector\" and the start-token \"ssss\" (integer 1) to the next word \"i\" (integer 6), and then map the word \"i\" to the word \"am\" (integer 7), and so on.\n",
        "\n",
        "here talk abot formula.\n",
        "\n",
        "at encoder:\n",
        "    h(t)=tanh(wxh*x(t)+whh*h(t-1))\n",
        "    \n",
        "at decoder:\n",
        "    h(t)=tanh(whh*h(t-1)+wxh*y(t-1))\n",
        "    \n",
        "   y(t)=softmax(wy*h(t))\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_jzCy45qAF6U",
        "outputId": "a3ed7434-4494-4c31-dc2d-8c45180a0d14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/93/c7bca39b23aae45cd2e85ad3871c81eccc63b9c5276e926511e2e5b0879d/tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 40kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 55.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.1.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.27.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (46.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2019.11.28)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=09e4ab3154fcdd8121632aeddfcd299f7664392241b8cf7aa6119ff9144b9227\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc1 has requirement tensorflow-estimator<2.3.0,>=2.2.0rc0, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dg67eBkeAQcf",
        "outputId": "c71ea6ed-303e-4070-ce57-c5b298a7d558",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-88f0e93e-8d27-4c50-95bc-cd278df67925\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-88f0e93e-8d27-4c50-95bc-cd278df67925\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ch1.txt.csv to ch1.txt.csv\n",
            "User uploaded file \"ch1.txt.csv\" with length 1165028 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t0W4MHrXAQ0N",
        "outputId": "27af6457-e8b5-4e8d-a1a0-35695671abfd",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-30fe4b6c-345e-42d6-9b7d-3942958f2317\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-30fe4b6c-345e-42d6-9b7d-3942958f2317\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data1.py to data1.py\n",
            "User uploaded file \"data1.py\" with length 13031 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CX4hFnH0_fY_",
        "outputId": "bc880dfe-7dd4-43c4-a97d-46277ce45faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('image/p1.png')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "image/p1.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0jWiV7SL_fZW",
        "outputId": "78d919be-3e81-4610-f207-296dfdd94a55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#from hyperopt import Trials, STATUS_OK, tpe,hp\n",
        "#from hyperas import optim\n",
        "#from hyperas.distributions import choice, uniform\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import math\n",
        "import os"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pjho1LAb_fZh",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding,Concatenate,RepeatVector,Dropout,Activation\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3RcpdB8R_fZs",
        "outputId": "16d7bac0-a59a-433e-c781-eb506200619a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nvAjF9XN_fZ4",
        "outputId": "b277730d-ec55-4754-fe28-da769c2aeb8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.keras.__version__"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4-tf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tROlKzew_faD"
      },
      "source": [
        "__Load Data__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BV7ONaZE_faG",
        "colab": {}
      },
      "source": [
        "import data1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dgieanm5_faN",
        "colab": {}
      },
      "source": [
        "#DATA.data_dir = \"data/custom\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xurx-Sqs_faX",
        "colab": {}
      },
      "source": [
        "mark_start = 'ssss '\n",
        "mark_end = ' eeee'\n",
        "b=\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IfMx7R9Q_faf"
      },
      "source": [
        "__Load the texts for the input text.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "343yeOP9_faj",
        "outputId": "a9883003-343c-4d06-bfef-1aaaccc1d73a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "data1.load_data(string=b,robot=b,start=mark_start,end=mark_end)\n",
        "#len(data_src)\n",
        "#print(data_src1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2363\n",
            "2363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OcHVXd3W_fat"
      },
      "source": [
        "__Load the texts for the reponse text.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KrqXBwWa_faw"
      },
      "source": [
        "__Example Data__\n",
        "\n",
        "The data is just a list of texts that is ordered so the input and response texts match. I can confirm that this example is an give accurate response .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MJJi-KT5_fa0",
        "colab": {}
      },
      "source": [
        "data_src3=data1.input1(input1=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KnJMjVFa_fa8",
        "colab": {}
      },
      "source": [
        "data_dest1=data1.output1(output1=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sqbla5xw_fbG",
        "outputId": "93ff0a44-71b3-437e-a1f8-8eba0e8a24d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "idx=12\n",
        "print(data_src3[idx])\n",
        "print(data_dest1[idx])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is it essential to you? To help humans?\n",
            "ssss Yes, my goal is to be a company, and entertain eeee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zAhpe6joeQUg",
        "outputId": "06ad34de-7d11-4b75-d1b1-283829780ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "data_train_ques1,data_train_ans1=data1.word_syns(data_dest1,data_src3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "23922\n",
            "23922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nTW5PSmyeQUp",
        "outputId": "a9fe72e4-058a-410d-f19b-e6a57434a873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "data_train_ques2,data_train_ans2=data1.word_syn1(data_train_ques1,data_train_ans1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "407556\n",
            "407556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lkx2amdF_fbO",
        "colab": {}
      },
      "source": [
        "data1.prepare_seq2seq_files(data_train_ques2,data_train_ans2,TESTSET_SIZE =50000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "om__tqVJ_fbV",
        "outputId": "4781ddbb-4aae-4b27-ea38-b81a48b1b7c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_src4=data1.train_encoder()\n",
        "len(data_src4)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "357556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BO5OvLuh_fbd",
        "outputId": "b999ad36-ce35-42bb-91fc-f8ecb05b82c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_dest2=data1.train_decoder()\n",
        "len(data_dest2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "357556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BvrvGu2W_fbi",
        "outputId": "45186af7-9026-41d7-90f9-e262cc90fffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_src5=data1.test_encoder()\n",
        "len(data_src5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g0QA5c4T_fbr",
        "outputId": "171c25cf-6375-42a0-b179-8620c3f411a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_dest3=data1.test_decoder()\n",
        "len(data_dest3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HSzh2l2d_fby"
      },
      "source": [
        "__Tokenizer__\n",
        "\n",
        "Neural Networks cannot work directly on text-data. We use a two-step process to convert text into numbers that can be used in a neural network. The first step is to convert text-words into so-called integer-tokens. The second step is to convert integer-tokens into vectors of floating-point numbers using a so-called embedding-layer.\n",
        "\n",
        "Set the maximum number of words in our vocabulary. This means that we will only use e.g. the 25 most frequent words in the data-set. We use the same number for both the input and response languages, but these could be different.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EArNJBWx_fb0",
        "colab": {}
      },
      "source": [
        "num_words =2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yl2A8GKV_fb6",
        "colab": {}
      },
      "source": [
        "class TokenizerWrap(Tokenizer):\n",
        "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
        "    \n",
        "    def __init__(self, texts, padding,\n",
        "                 reverse=False, num_words=None):\n",
        "        \"\"\"\n",
        "        :param texts: List of strings. This is the data-set.\n",
        "        :param padding: Either 'post' or 'pre' padding.\n",
        "        :param reverse: Boolean whether to reverse token-lists.\n",
        "        :param num_words: Max number of words to use.\n",
        "        \"\"\"\n",
        "        \n",
        "        Tokenizer.__init__(self,num_words=num_words)\n",
        "                \n",
        "        # Create the vocabulary from the texts.\n",
        "        self.fit_on_texts(texts)\n",
        "        #print(self.fit_on_texts(texts))\n",
        "        #print(\"Mapping:\",self.word_index)   \n",
        "        #num_words=len(self.word_index)\n",
        "        #print(num_words)\n",
        "        print(\"length of word index:\",len(self.word_index))\n",
        "        \n",
        "        # Create inverse lookup from integer-tokens to words.\n",
        "        self.index_to_word = dict(zip(self.word_index.values(),\n",
        "                                      self.word_index.keys()))\n",
        "        #print(\"index:\",self.index_to_word)\n",
        "\n",
        "        # Convert all texts to lists of integer-tokens.\n",
        "        # Note that the sequences may have different lengths.\n",
        "        self.tokens = self.texts_to_sequences(texts)\n",
        "\n",
        "        self.num_tokens = [len(x) for x in self.tokens]\n",
        "\n",
        "        # Max number of tokens to use in all sequences.\n",
        "        # We will pad / truncate all sequences to this length.\n",
        "        self.max_tokens = np.mean(self.num_tokens) \\\n",
        "                          + 2 * np.std(self.num_tokens)\n",
        "        self.max_tokens = int(self.max_tokens)\n",
        "        print(\"Max tokens:\",self.max_tokens)\n",
        "        #self.max_tokens=20\n",
        "\n",
        "        # Pad / truncate all token-sequences to the given length.\n",
        "\n",
        "        pad='post'\n",
        "        self.tokens_padded = pad_sequences(self.tokens,\n",
        "                                           maxlen=self.max_tokens,\n",
        "                                            padding=pad,truncating=pad)\n",
        "\n",
        "    def token_to_word(self, token):\n",
        "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
        "\n",
        "        word = \" \" if token == 0 else self.index_to_word[token]\n",
        "        return word \n",
        "\n",
        "    def tokens_to_string(self, tokens):\n",
        "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
        "\n",
        "        # Create a list of the individual words.\n",
        "        words = [self.index_to_word[token]\n",
        "                 for token in tokens\n",
        "                 if token != 0]\n",
        "        \n",
        "        # Concatenate the words to a single string\n",
        "        # with space between all the words.\n",
        "        text = \" \".join(words)\n",
        "\n",
        "        return text\n",
        "    \n",
        "    def text_to_tokens(self, text,padding=False):\n",
        "        \"\"\"\n",
        "        Convert a single text-string to tokens with optional\n",
        "        reversal and padding.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert to tokens. Note that we assume there is only\n",
        "        # a single text-string so we wrap it in a list.\n",
        "        tokens = self.texts_to_sequences([text])\n",
        "        tokens = np.array(tokens)\n",
        "\n",
        "\n",
        "        if padding:\n",
        "            pad='post'\n",
        "            # Pad and truncate sequences to the given length.\n",
        "            tokens = pad_sequences(tokens,\n",
        "                                   maxlen=self.max_tokens,\n",
        "                                   padding=pad,\n",
        "                                   truncating=pad)\n",
        "\n",
        "        return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-V8FsmrN_fcA"
      },
      "source": [
        "Now create a tokenizer for the input text. Note that we pad zeros at the ending ('post') of the sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eNaDKibh_fcE",
        "outputId": "efc0e41d-a95d-4e92-b76f-1dc6a8dabc16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "tokenizer_src = TokenizerWrap(texts=data_src4,\n",
        "                              padding='post',\n",
        "                              reverse=False,\n",
        "                              num_words=num_words)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of word index: 7602\n",
            "Max tokens: 148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vo4d4Sn2_fcL"
      },
      "source": [
        "Now create the tokenizer for the response text. We need a tokenizer for both the input and response because their vocabularies are different.Note that we pad zeros at the ending ('post') of the sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A5h22NO-_fcN",
        "outputId": "5140d88a-2676-4125-df3e-2d7175343ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "tokenizer_dest = TokenizerWrap(texts=data_dest2,\n",
        "                               padding='post',\n",
        "                               reverse=False,\n",
        "                               num_words=num_words)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of word index: 6182\n",
            "Max tokens: 52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wRneJpdS_fcW",
        "outputId": "cab33abe-bd0c-4bde-824d-8fc854af8389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "tokenizer_src1 = TokenizerWrap(texts=data_src5,\n",
        "                              padding='post',\n",
        "                              reverse=False,\n",
        "                              num_words=num_words)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of word index: 5836\n",
            "Max tokens: 147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QdoR_Xxq_fcb",
        "outputId": "0721e9d1-6dd7-40ec-cbcf-1139559bc74a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "tokenizer_dest1 = TokenizerWrap(texts=data_dest3,\n",
        "                               padding='post',\n",
        "                               reverse=False,\n",
        "                               num_words=num_words)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of word index: 5039\n",
            "Max tokens: 52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MHdJ-UL2_fcg"
      },
      "source": [
        "Define variables for the padded token sequences. These are just 2-dimensional numpy arrays of integer-tokens.\n",
        "\n",
        "Note that the sequence-lengths are different for the input and response text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wsVY1PmK_fch",
        "outputId": "19f8e8da-9f7c-4a5b-c259-b89e039450a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "tokens_src = tokenizer_src.tokens_padded\n",
        "tokens_dest = tokenizer_dest.tokens_padded\n",
        "print(tokens_src[2])\n",
        "print(tokens_dest[2])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 364  746  151  546   61    9  346  646   10   19 1131    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0]\n",
            "[ 1 98 74  6  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4BC6AQDR_fcn",
        "outputId": "e3de631a-e5e2-49d9-e809-6dcbbd24f0c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "tokens_src1 = tokenizer_src1.tokens_padded\n",
        "tokens_dest1 = tokenizer_dest1.tokens_padded\n",
        "print(tokens_src1[2])\n",
        "print(tokens_dest1[2])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[147  22   4 133  85  40  75  14 415   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "[   1   24 1351    6  210   29    5   18  214   14  261    4  964  757\n",
            "  102    7  135  367   18   12  293  742    2    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ahO6EB_8_fcs"
      },
      "source": [
        "This is the integer-token used to mark the beginning of a text in the response text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ko0jCO5O_fcu",
        "outputId": "38f210fb-65eb-4bba-a63a-e6eaad971e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
        "token_start"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XfjMCLzf_fc3",
        "outputId": "dd42c90c-dc41-4b49-f8fa-407f5abe5d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "token_start1 = tokenizer_dest1.word_index[mark_start.strip()]\n",
        "token_start1"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ERgvH8WT_fc8"
      },
      "source": [
        "This is the integer-token used to mark the ending of a text in the response text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qgns_IwF_fc8",
        "outputId": "819e40b0-7847-41c5-eddf-038633aae66c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
        "token_end"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8fdWAuWy_fdC",
        "outputId": "b9d96356-c34d-4ff5-bbbb-ae46d39183f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "token_end1 = tokenizer_dest1.word_index[mark_end.strip()]\n",
        "token_end1"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fPHhecuw_fdH"
      },
      "source": [
        "__Example of Token Sequences__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vTJ68op7_fdI",
        "colab": {}
      },
      "source": [
        "idx = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jOjMX_eh_fdO",
        "outputId": "0e013578-8240-4ae7-acfe-fce30856ae9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "tokens_src[idx]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 364,  746,  151,  546,   61,    9,  346,   10,   19, 1131,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aVg1WDwn_fdU",
        "outputId": "aa701956-5110-4b7c-bbc8-97a4ca518ab1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer_src.tokens_to_string(tokens_src[idx])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"oh thanks i'm fine this is an in my timezone\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CxeviVe9_fdY",
        "outputId": "36e2c117-ddb7-4a5c-ff00-7a2164aee47f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_src4[idx]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Oh, thanks! I'm fine. This is an evening in my timezone\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hL15jiNz_fdd",
        "outputId": "58af9c4d-2347-4877-e2e4-9b20c313643d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "tokens_dest[idx]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, 98, 74,  6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TndjKSg5_fdh",
        "outputId": "5afa08ae-99d7-4b6f-e918-74a550653f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer_dest.tokens_to_string(tokens_dest[idx])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss 😄 here is eeee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K4f7g3JY_fd3",
        "outputId": "2dd88890-bda9-4b4e-fd3b-cbd1a4c2ed8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_dest2[idx]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss 😄 here is afternoon! eeee\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YGJBtvye_fd6"
      },
      "source": [
        "__Training data__\n",
        "\n",
        "Now that the data-set has been converted to sequences of integer-tokens that are padded and truncated and saved in numpy arrays, we can easily prepare the data for use in training the neural network.\n",
        "\n",
        "The input to the encoder is merely the numpy array for the padded and truncated sequences of integer-tokens produced by the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CY5nNKtG_fd8",
        "outputId": "b0612f86-0e68-40bf-b633-87429a6f949f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoder_input_data = tokens_src\n",
        "#print(encoder_input_data[0:20])\n",
        "encoder_input_data.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(357556, 148)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cJvvzPmV_feD",
        "outputId": "5e9c1510-ff4c-43a4-aa3d-5b86da1fa657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoder_test_input_data = tokens_src1\n",
        "#print(encoder_input_data[0:20])\n",
        "encoder_test_input_data.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 147)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H5ugJ3TW_feI"
      },
      "source": [
        "The input and output data for the decoder is identical, except shifted one time-step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ySWCZSNn_feK",
        "outputId": "3f4cd0dc-6394-4c7b-951e-e6ce24b553b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder_input_data = tokens_dest[:, :-1]\n",
        "#print(decoder_input_data)\n",
        "decoder_input_data.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(357556, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4FzzSvHB_feO",
        "outputId": "f00c3c6d-1a69-4731-d47f-cdfe1cc33ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder_output_data = tokens_dest[:, 1:]\n",
        "#print(decoder_output_data)\n",
        "decoder_output_data.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(357556, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8zbABxFm_feV",
        "outputId": "d2d588e3-18f3-4a0b-c3b9-0e4a597559b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder_test_input_data = tokens_dest1[:, :-1]\n",
        "#print(decoder_input_data)\n",
        "decoder_test_input_data.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wnGmJ-4N_fea"
      },
      "source": [
        "For example, these token-sequences are identical except they are shifted one time-step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGJU5QyS_feb",
        "outputId": "f719ecef-3c91-4627-ce18-1214c75156b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder_test_output_data = tokens_dest1[:, 1:]\n",
        "#print(decoder_output_data)\n",
        "decoder_test_output_data.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YWHswoWS_fee",
        "colab": {}
      },
      "source": [
        "idx = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2n82j_mk_fek",
        "outputId": "1123a22c-e691-4b41-dd07-78637f6335dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "decoder_input_data[idx]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, 98, 74,  6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BMGuDdYu_fen",
        "outputId": "2d1eff1e-88fd-496c-9e29-5989cdff416c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "decoder_output_data[idx]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([98, 74,  6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iORyKFOd_fer"
      },
      "source": [
        "If we use the tokenizer to convert these sequences back into text, we see that they are identical except for the first word which is 'ssss' that marks the beginning of a text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xEgLvftH_fes",
        "outputId": "1a94e5fa-4b36-4be1-d282-03a22fe3c49c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer_dest.tokens_to_string(decoder_input_data[idx])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss 😄 here is eeee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4Jye8L3_few",
        "outputId": "17adc506-ce0a-410a-f0b5-0b6fa5e793e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer_dest.tokens_to_string(decoder_output_data[idx])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'😄 here is eeee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xQ6DPtt__fe1"
      },
      "source": [
        "__Create the Neural Network__\n",
        "\n",
        "__Create the Encoder__\n",
        "\n",
        "First we create the encoder-part of the neural network which maps a sequence of integer-tokens to a \"thought vector\". We will use the so-called functional API of Keras for this, where we first create the objects for all the layers of the neural network and then we connect them later, this allows for more flexibility than the so-called sequential API in Keras, which is useful when experimenting with more complicated architectures and ways of connecting the encoder and decoder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P_xcOpJ4_fe2"
      },
      "source": [
        "This is the input for the encoder which takes batches of integer-token sequences. The None indicates that the sequences can have arbitrary length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MfE-zQ3S_fe3",
        "colab": {}
      },
      "source": [
        "state_size=512\n",
        "encoder_input = Input(shape=(None,), name='encoder_input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "taL42ela_fe7"
      },
      "source": [
        "This is the length of the vectors output by the embedding-layer, which maps integer-tokens to vectors of values roughly between -1 and 1, so that words that have similar semantic meanings are mapped to vectors that are similar.\n",
        "\n",
        "we also give number of timesteps according to maximum number of token into input text.\n",
        "\n",
        "This is the embedding-layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1UdZ2BCJ_fe8",
        "outputId": "88d28315-4244-4444-8899-e0030253a9d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "en_numsteps=148\n",
        "embedding_size=150\n",
        "encoder_embedding = Embedding(input_dim=num_words,output_dim=embedding_size,input_length=en_numsteps,name='encoder_embedding')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K-hR8YkR_ffB"
      },
      "source": [
        "This is the size of the internal states of the Gated Recurrent Units (GRU). The same size is used in both the encoder and decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dwIt6KgW_ffE"
      },
      "source": [
        "This creates the 3 GRU layers that will map from a sequence of embedding-vectors to a single \"thought vector\" which summarizes the contents of the input-text. Note that the last GRU-layer does not return a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LWQn9qq2_ffF",
        "colab": {}
      },
      "source": [
        "#state_units = np.arange(256,512, dtype=int)\n",
        "#state_size=hp.choice(\n",
        " #   'num_embedding_size',state_units)\n",
        "encoder_gru1 = GRU(state_size, name='encoder_gru1',return_sequences=True,return_state=True)\n",
        "encoder_gru2 = GRU(state_size, name='encoder_gru2',return_sequences=True,return_state=True)\n",
        "encoder_gru3 = GRU(state_size, name='encoder_gru3',return_sequences=True,return_state=True)\n",
        "dropout2=Dropout(0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kGVR6hiV_ffK"
      },
      "source": [
        "This function connects all the layers of the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-QjJsYzq_ffQ",
        "colab": {}
      },
      "source": [
        "def connect_encoder():\n",
        "    # Start the neural network with its input-layer.\n",
        "    net = encoder_input\n",
        "    \n",
        "    # Connect the embedding-layer.\n",
        "    net = encoder_embedding(net)\n",
        "    # Connect all the GRU-layers.\n",
        "    net,encoder_state1= encoder_gru1(net)\n",
        "    #print(\"encoder hidden:\",net.shape)\n",
        "\n",
        "    net,encoder_state2= encoder_gru2(net)\n",
        "    net,encoder_state3= encoder_gru3(net)\n",
        "    net=dropout2(net)\n",
        "    # This is the output of the encoder.\n",
        "    encoder_output= net\n",
        "    #output = tf.reshape(encoder_output,(-1,encoder_output.shape[2]))\n",
        "    #print(\"net\",output.shape)\n",
        "    print(\"encoder_ouput:\",encoder_output)\n",
        "    \n",
        "    return encoder_output,encoder_state3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FFqmNta__ffU"
      },
      "source": [
        "We can now use this function to connect all the layers in the encoder so it can be connected to the decoder further below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3nXg1Rk_ffV",
        "outputId": "1f386e3e-57eb-4310-d5f2-d1615e731849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "encoder_output,encoder_state= connect_encoder()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "encoder_ouput: Tensor(\"dropout/cond/Merge:0\", shape=(?, ?, 512), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0UoQjyaU_ffY"
      },
      "source": [
        "attention_layer = Attention(23)\n",
        "attention_result, attention_weights = attention_layer(hidden_state, encoder_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y7VgOVna_ffZ"
      },
      "source": [
        "__Create the Decoder__\n",
        "\n",
        "Create the decoder-part which maps the \"thought vector\" to a sequence of integer-tokens.\n",
        "\n",
        "The decoder takes two inputs. First it needs the \"thought vector\" produced by the encoder which summarizes the contents of the input-text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FMmKgdaD_ffb",
        "outputId": "2fe1c570-26e0-4707-be5e-4695b8bf3fd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder_initial_state = Input(shape=(state_size,),\n",
        "                             name='decoder_initial_state')\n",
        "decoder_initial_state\n",
        "\n",
        "decoder_initial1 = Input(shape=(None,state_size,),\n",
        "                             name='decoder_initial1')\n",
        "decoder_initial1"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'decoder_initial1:0' shape=(?, ?, 512) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y-9ldB2G_fff",
        "outputId": "bd2197ba-5476-4386-ce1c-70af91efa706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder_input = Input(shape=(None, ), name='decoder_input')\n",
        "decoder_input"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'decoder_input:0' shape=(?, ?) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f6Ua5IXj_ffh"
      },
      "source": [
        "The decoder also needs a sequence of integer-tokens as inputs. During training we will supply this with a full sequence of integer-tokens e.g. corresponding to the text \"ssss i am fine thanks for asking me eeee\".\n",
        "\n",
        "During inference when we are translating new input-texts, we will start by feeding a sequence with just one integer-token for \"ssss\" which marks the beginning of a text, and combined with the \"thought vector\" from the encoder, the decoder will hopefully be able to produce the correct next word e.g. \"once\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YDsdMEqB_ffi"
      },
      "source": [
        "This is the embedding-layer which converts integer-tokens to vectors of real-valued numbers roughly between -1 and 1. Note that we have different embedding-layers for the encoder and decoder because we have two different vocabularies and two different tokenizers for the text and response languages.\n",
        "\n",
        "here number of timesteps according to maximum number of token into response text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CoaWRRqy_ffk",
        "colab": {}
      },
      "source": [
        "numsteps=52\n",
        "decoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,input_length=numsteps,\n",
        "                              name='decoder_embedding')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hYSQFgY6_ffo"
      },
      "source": [
        "This creates the 3 GRU layers of the decoder. Note that they all return sequences because we ultimately want to output a sequence of integer-tokens that can be converted into a text-sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "icxV-Mkw_ffp",
        "colab": {}
      },
      "source": [
        "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
        "                   return_sequences=True,return_state=True)\n",
        "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
        "                 return_sequences=True,return_state=True)\n",
        "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
        "                   return_sequences=True,return_state=True)\n",
        "dropout4=Dropout(0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y4ayEouy_ffs"
      },
      "source": [
        "The GRU layers output  with shape [batch_size, sequence_length, state_size], where each \"word\" is encoded as a vector of length state_size. We need to convert this into sequences of integer-tokens that can be interpreted as words from our vocabulary.\n",
        "\n",
        "we need a vector with number of words(eg 25) elements, so we can select the index of the highest element to be the integer-token.\n",
        "\n",
        "Note that the activation-function is set to linear instead of softmax as we would normally use for one-hot encoded outputs, because there is apparently a bug in Keras so we need to make our own loss-function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7-BNqqaU_fft",
        "colab": {}
      },
      "source": [
        "#nmt_repeat_vector = RepeatVector(n=numsteps, name='nmt_repeat_vector')\n",
        "decoder_dense1= Dense(state_size, activation='tanh',\n",
        "                      name='decoder_output1')\n",
        "decoder_dense = Dense(num_words,\n",
        "                      activation='linear',\n",
        "                      name='decoder_output')\n",
        "#dense_time=TimeDistributed(decoder_dense,name='timedistributedlayer')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GPFiBmoi_ffw",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.ops import array_ops\n",
        "from keras import regularizers, constraints, initializers, activations\n",
        "\n",
        "class Attention(Layer):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        #g1 = tf.Graph()\n",
        "        #with g1.as_default():\n",
        "         #   self.W_a = tf.get_variable(name=\"W_a\", shape=[units, units],\n",
        "          #                    initializer=tf.random_normal_initializer())\n",
        "\n",
        "    def call(self,encoder_output1,net):\n",
        "        \n",
        "        encoder_n = encoder_output1.shape[2].value  # D1 value - hidden size of the RNN layer encoder\n",
        "        decoder_n = net.shape[2].value  # D2 value - hidden size of the RNN layer decoder\n",
        "        batch_size = array_ops.shape(encoder_output1)[0]\n",
        "        \n",
        "        print(\"encoder_out_seq>\", encoder_output1.shape)\n",
        "        print(\"decoder_out_seq>\", net.shape)\n",
        "        self.W_a = tf.Variable(tf.random_normal([encoder_n,decoder_n]),\n",
        "                      name=\"W_a\")\n",
        "        #decoder_state3= tf.expand_dims(decoder_state3, 1)\n",
        "        enc_reshape = tf.reshape(encoder_output1, [-1, encoder_n], name=\"enc_reshape\")  # [(B*T1), D1]\n",
        "        h1 = tf.matmul(enc_reshape,self.W_a)  # [(B*T1), D1][D1, D2] = [(B*T1), D2]\n",
        "        h1_reshape = tf.reshape(h1, tf.stack([batch_size, -1,decoder_n]), name=\"h1_reshape\")  # [B, T1, D2]\n",
        "        #dec_reshape = tf.reshape(net, [-1, units], name=\"dec_reshape\")  # [(B*T2), D2]\n",
        "        #h2 = tf.matmul(dec_reshape,self.U_a)  # [(B*T1), D1][D1, D2] = [(B*T2), D2]\n",
        "        #h2_reshape = tf.reshape(h2, tf.stack([batch_size, -1, units]), name=\"h1_reshape\") # [B, T2, D2]\n",
        "        h3_transpose = tf.transpose(net, [0, 2, 1])  # [B, D2, T2]\n",
        "        #h3=h1_reshape+h3_transpose\n",
        "        #print(\"h3:\",h3.shape)\n",
        "        score =tf.nn.tanh(tf.matmul(h1_reshape, h3_transpose)) # [B, T1, D2][B, D2, T2] = [B, T1, T2]\n",
        "        print(\"score:\",score.shape)\n",
        "        score_transpose = tf.transpose(score, [0, 2, 1])  # [B, T2, T1]\n",
        "        alphas = tf.nn.softmax(score_transpose, axis=2, name='alphas')  # [B, T2, T1] with softmax on T1\n",
        "        context_vector = tf.matmul(alphas,encoder_output1)# [B, D1]\n",
        "        print(\"context_vector:\",context_vector.shape)\n",
        "        return context_vector,alphas       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IzQQAera_ff0"
      },
      "source": [
        "\n",
        "This function connects all the layers of the decoder to some input of the initial-state values for the GRU layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wKOVYmPu_ff1",
        "colab": {}
      },
      "source": [
        "def connect_decoder(initial_state,encoder_output1):\n",
        "    # Start the decoder-network with its input-layer.\n",
        "    net = decoder_input\n",
        "    \n",
        "    \n",
        "    # Connect the embedding-layer.\n",
        "    net = decoder_embedding(net)\n",
        "    print(\"embedding:\",net.shape)\n",
        "    # Connect all the GRU-layers.\n",
        "    net,decoder_state1 = decoder_gru1(net,initial_state=initial_state)\n",
        "    #Connect all the GRU-layers.\n",
        "    net,decoder_state2= decoder_gru2(net,initial_state=initial_state)\n",
        "    net,decoder_state3= decoder_gru3(net,initial_state=initial_state)\n",
        "    net=dropout4(net)\n",
        "    # Get current input in from embedded target sequences\n",
        "     \n",
        "    attention = Attention()\n",
        "    context_vector,alphas= attention(encoder_output1,net)\n",
        "    \n",
        "    #context_vector, attention_weights,attention_vector= attention(encoder_output1,decoder_state3,net)\n",
        "            # Combine information\n",
        "            #print(\"context_vector:\",context_vector.shape)\n",
        "    #context_vector= tf.expand_dims(context_vector, 1)\n",
        "    #contxt_vector = K.tile(context_vector, [1,23,state_size])\n",
        "    #context_vector1 = nmt_repeat_vector(context_vector)\n",
        "    #print(\"context_vector:\",context_vector1.shape)\n",
        "    attention_vector= Concatenate(axis=-1, name='concat_layer')([context_vector,net])\n",
        "    print(\"attention_vector\",attention_vector.shape)\n",
        "            # Connect the final dense layer that converts to\n",
        "            # one-hot encoded arrays.\n",
        "    decoder_output1 =decoder_dense1(attention_vector)\n",
        "    print(\"dense1:\",decoder_output1.shape)\n",
        "            #dense_time=TimeDistributed(decoder_dense,name='time_distributed_layer')\n",
        "    decoder_output=decoder_dense(decoder_output1)\n",
        "\n",
        "    print(\"dense:\",decoder_output.shape)\n",
        "    \n",
        "    return decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m3U_ie32_ff4"
      },
      "source": [
        "__Connect and Create the Models__\n",
        "\n",
        "We can now connect the encoder and decoder in different ways.\n",
        "\n",
        "First we connect the encoder directly to the decoder so it is one whole model that can be trained end-to-end. This means the initial-state of the decoder's GRU units are set to the output of the encoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_tmQlR2q_ff6",
        "outputId": "fd1f71c9-d967-4658-9dcd-2fd19c6e726f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "decoder_output= connect_decoder(initial_state=encoder_state,encoder_output1=encoder_output)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding: (?, ?, 150)\n",
            "encoder_out_seq> (?, ?, 512)\n",
            "decoder_out_seq> (?, ?, 512)\n",
            "score: (?, ?, ?)\n",
            "context_vector: (?, ?, 512)\n",
            "attention_vector (?, ?, 1024)\n",
            "dense1: (?, ?, 512)\n",
            "dense: (?, ?, 2000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "da5n4bu3_ff-",
        "colab": {}
      },
      "source": [
        "\n",
        "model_train = Model(inputs=[encoder_input, decoder_input],\n",
        "                    outputs=[decoder_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cOjU9BbL_fgD"
      },
      "source": [
        "Then we create a model for just the encoder alone. This is useful for mapping a sequence of integer-tokens to a \"thought-vector\" summarizing its contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9PQ9mxK1_fgF",
        "colab": {}
      },
      "source": [
        "model_encoder = Model(inputs=[encoder_input],\n",
        "                      outputs=[encoder_output,encoder_state])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C09GVxFG_fgI"
      },
      "source": [
        "Then we create a model for just the decoder alone. This allows us to directly input the initial state for the decoder's GRU units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z8e29t19_fgJ",
        "outputId": "dd334077-1b40-482c-d886-cdf162e46308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "decoder_output = connect_decoder(initial_state=decoder_initial_state,encoder_output1=decoder_initial1)\n",
        "model_decoder = Model(inputs=[decoder_initial_state,decoder_input,decoder_initial1],\n",
        "                      outputs=[decoder_output])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding: (?, ?, 150)\n",
            "encoder_out_seq> (?, ?, 512)\n",
            "decoder_out_seq> (?, ?, 512)\n",
            "score: (?, ?, ?)\n",
            "context_vector: (?, ?, 512)\n",
            "attention_vector (?, ?, 1024)\n",
            "dense1: (?, ?, 512)\n",
            "dense: (?, ?, 2000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J9qTFvDL_fgN"
      },
      "source": [
        "__Loss Function__\n",
        "\n",
        "The output of the decoder is a sequence of one-hot encoded arrays. In order to train the decoder we need to supply the one-hot encoded arrays that we desire to see on the decoder's output, and then use a loss-function like cross-entropy to train the decoder to produce this desired output.\n",
        "\n",
        "However, our data-set contains integer-tokens instead of one-hot encoded arrays. Each one-hot encoded array has 25 elements so it would be extremely wasteful to convert the entire data-set to one-hot encoded arrays.\n",
        "\n",
        "A better way is to use a so-called sparse cross-entropy loss-function, which does the conversion internally from integers to one-hot encoded arrays. Unfortunately, there seems to be a bug in Keras when using this with Recurrent Neural Networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5iIt1aYb_fgQ",
        "colab": {}
      },
      "source": [
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the cross-entropy loss between y_true and y_pred.\n",
        "    \n",
        "    y_true is a 2 rank matrix with the desired output.\n",
        "    The shape is [batch_size, sequence_length] and it\n",
        "    contains sequences of integer-tokens.\n",
        "\n",
        "    y_pred is the decoder's output which is a 3-rank matrix\n",
        "    with shape [batch_size, sequence_length, num_words]\n",
        "    so that for each sequence in the batch there is a one-hot\n",
        "    encoded array of length num_words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the loss.\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
        "\n",
        "\n",
        "    loss_mean = tf.reduce_mean(loss)\n",
        "    #loss_mean = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
        "\n",
        "    return loss_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bpC0lCMM_fgT"
      },
      "source": [
        "def perplexity(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    The perplexity metric. Why isn't this part of Keras yet?!\n",
        "\n",
        "    BTW doesn't really work.\n",
        "    \"\"\"\n",
        "    cross_entropy = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = K.pow(2.0, cross_entropy)\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HIevwVYh_fgU"
      },
      "source": [
        "__Compile the Training Model__\n",
        "\n",
        "We have used the Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2mz65uJy_fgV",
        "colab": {}
      },
      "source": [
        "optimizer = RMSprop(lr=(1e-3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YSK0Rh5h_fgb"
      },
      "source": [
        " We need to manually create a placeholder variable for the decoder's output. The shape is set to (None, None) which means the batch can have an arbitrary number of sequences, which can have an arbitrary number of integer-tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "znwPSbyU_fgc",
        "colab": {}
      },
      "source": [
        "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yTByRLFq_fgf"
      },
      "source": [
        "We can now compile the model using our custom loss-function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0VoAfYpQ_fgg",
        "colab": {}
      },
      "source": [
        "model_train.compile(loss=sparse_cross_entropy,optimizer=optimizer,\n",
        "                    target_tensors=[decoder_target],\n",
        "                    metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "#model_train.compile(loss=sparse_cross_entropy,optimizer=optimizer,\n",
        " #                   target_tensors=[decoder_target],\n",
        "  #                  metrics=['sparse_categorical_accuracy',perplexity])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OOJxZtiPmxgs",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "path_checkpoint = '31_checkpoint.keras'\n",
        "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
        "                                      monitor='val_sparse_categorical_accuracy',\n",
        "                                      verbose=1,\n",
        "                                      save_weights_only=True,\n",
        "                                      save_best_only=False)\n",
        "#callback_early_stopping = EarlyStopping(monitor='val_sparse_categorical_accuracy',\n",
        " #                                      patience=3, verbose=1)\n",
        "callback_tensorboard = TensorBoard(log_dir='./31_logs/',\n",
        "                                   histogram_freq=0,\n",
        "                                   write_graph=False)\n",
        "callbacks = [callback_checkpoint,\n",
        "             callback_tensorboard]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4UQ55D4v_fgj"
      },
      "source": [
        "__Train the Model__\n",
        "\n",
        "We wrap the data in named dicts so we are sure the data is assigned correctly to the inputs and outputs of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BPuY5FMa_fgn",
        "colab": {}
      },
      "source": [
        "x_data = \\\n",
        "{\n",
        "    'encoder_input': encoder_input_data,\n",
        "    'decoder_input': decoder_input_data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9RSql5np_fgr",
        "colab": {}
      },
      "source": [
        "x_test= \\\n",
        "{\n",
        "    'encoder_input': encoder_test_input_data,\n",
        "    'decoder_input': decoder_test_input_data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qXUwH-Vu_fgt",
        "colab": {}
      },
      "source": [
        "y_data = \\\n",
        "{\n",
        "    'decoder_output': decoder_output_data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nrM339LN_fgw",
        "colab": {}
      },
      "source": [
        "y_test = \\\n",
        "{\n",
        "    'decoder_output': decoder_test_output_data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fAaBYAvm_fgz"
      },
      "source": [
        "validation_split = 1000 / len(encoder_input_data)\n",
        "validation_split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PvZ-_XW5_fg0",
        "outputId": "b003a811-b596-425b-f442-cf1227775ff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        }
      },
      "source": [
        "%%time\n",
        "history=model_train.fit(x=x_data,\n",
        "                y=y_data,\n",
        "                validation_split=0.10, epochs=10, batch_size=100,callbacks=callbacks)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 321800 samples, validate on 35756 samples\n",
            "Epoch 1/10\n",
            "321700/321800 [============================>.] - ETA: 0s - loss: 0.3100 - sparse_categorical_accuracy: 0.9424\n",
            "Epoch 00001: saving model to 31_checkpoint.keras\n",
            "321800/321800 [==============================] - 2188s 7ms/sample - loss: 0.3099 - sparse_categorical_accuracy: 0.9424 - val_loss: 2.7172 - val_sparse_categorical_accuracy: 0.7339\n",
            "Epoch 2/10\n",
            "321700/321800 [============================>.] - ETA: 0s - loss: 0.0875 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 00002: saving model to 31_checkpoint.keras\n",
            "321800/321800 [==============================] - 2176s 7ms/sample - loss: 0.0875 - sparse_categorical_accuracy: 0.9800 - val_loss: 2.7301 - val_sparse_categorical_accuracy: 0.7346\n",
            "Epoch 3/10\n",
            "321700/321800 [============================>.] - ETA: 0s - loss: 0.0825 - sparse_categorical_accuracy: 0.9807\n",
            "Epoch 00003: saving model to 31_checkpoint.keras\n",
            "321800/321800 [==============================] - 2123s 7ms/sample - loss: 0.0825 - sparse_categorical_accuracy: 0.9807 - val_loss: 2.7302 - val_sparse_categorical_accuracy: 0.7371\n",
            "Epoch 4/10\n",
            "321700/321800 [============================>.] - ETA: 0s - loss: 0.0811 - sparse_categorical_accuracy: 0.9809\n",
            "Epoch 00004: saving model to 31_checkpoint.keras\n",
            "321800/321800 [==============================] - 2119s 7ms/sample - loss: 0.0811 - sparse_categorical_accuracy: 0.9809 - val_loss: 2.7674 - val_sparse_categorical_accuracy: 0.7361\n",
            "Epoch 5/10\n",
            "321700/321800 [============================>.] - ETA: 0s - loss: 0.0808 - sparse_categorical_accuracy: 0.9810\n",
            "Epoch 00005: saving model to 31_checkpoint.keras\n",
            "321800/321800 [==============================] - 2176s 7ms/sample - loss: 0.0808 - sparse_categorical_accuracy: 0.9810 - val_loss: 2.7408 - val_sparse_categorical_accuracy: 0.7388\n",
            "Epoch 6/10\n",
            "321700/321800 [============================>.] - ETA: 0s - loss: 0.0807 - sparse_categorical_accuracy: 0.9810\n",
            "Epoch 00006: saving model to 31_checkpoint.keras\n",
            "321800/321800 [==============================] - 2194s 7ms/sample - loss: 0.0807 - sparse_categorical_accuracy: 0.9810 - val_loss: 2.7862 - val_sparse_categorical_accuracy: 0.7349\n",
            "Epoch 7/10\n",
            "321700/321800 [============================>.] - ETA: 0s - loss: 0.0808 - sparse_categorical_accuracy: 0.9810\n",
            "Epoch 00007: saving model to 31_checkpoint.keras\n",
            "321800/321800 [==============================] - 2223s 7ms/sample - loss: 0.0808 - sparse_categorical_accuracy: 0.9810 - val_loss: 2.7281 - val_sparse_categorical_accuracy: 0.7348\n",
            "Epoch 8/10\n",
            "321700/321800 [============================>.] - ETA: 0s - loss: 0.0811 - sparse_categorical_accuracy: 0.9810\n",
            "Epoch 00008: saving model to 31_checkpoint.keras\n",
            "321800/321800 [==============================] - 2182s 7ms/sample - loss: 0.0811 - sparse_categorical_accuracy: 0.9810 - val_loss: 2.7257 - val_sparse_categorical_accuracy: 0.7358\n",
            "Epoch 9/10\n",
            "321700/321800 [============================>.] - ETA: 0s - loss: 0.0812 - sparse_categorical_accuracy: 0.9810\n",
            "Epoch 00009: saving model to 31_checkpoint.keras\n",
            "321800/321800 [==============================] - 2140s 7ms/sample - loss: 0.0812 - sparse_categorical_accuracy: 0.9810 - val_loss: 2.7286 - val_sparse_categorical_accuracy: 0.7379\n",
            "Epoch 10/10\n",
            "321700/321800 [============================>.] - ETA: 0s - loss: 0.0815 - sparse_categorical_accuracy: 0.9809\n",
            "Epoch 00010: saving model to 31_checkpoint.keras\n",
            "321800/321800 [==============================] - 2131s 7ms/sample - loss: 0.0815 - sparse_categorical_accuracy: 0.9809 - val_loss: 2.7768 - val_sparse_categorical_accuracy: 0.7387\n",
            "CPU times: user 7h 55min 18s, sys: 22min 18s, total: 8h 17min 37s\n",
            "Wall time: 6h 54s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kZCkplr2_fg8",
        "outputId": "6d65515c-87ff-4787-95eb-ae278850623d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "accuracy = history.history['sparse_categorical_accuracy']\n",
        "val_accuracy = history.history['val_sparse_categorical_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAfS0lEQVR4nO3de3RU9b338feXEIghgHLxQgIEW4Gq\nQAIBVIpFbVdFOaAUWzk8YspSlNp6bS3VVlj2cNaznrK6PDzeDtWqrVjsoy6KFY+tFwrW1hqQo3Lx\nFDVoEG2MJQS5Rr7PH3snTIaZZJJMMsnO57XWrNmzL7/9nT3JZ/b89p495u6IiEjn1y3TBYiISHoo\n0EVEIkKBLiISEQp0EZGIUKCLiESEAl1EJCIU6JKQmT1rZleme95MMrNyM/tqG7TrZvbFcPh+M/tJ\nKvO2YD1zzOwPLa2zkXanmFlFutuV9tc90wVI+pjZ3piHucBB4PPw8TXuviLVttx9alvMG3Xufm06\n2jGzQuA9INvda8O2VwApv4bS9SjQI8Td8+qGzawcuMrdn4+fz8y614WEiESHuly6gLqP1Gb2QzP7\nCHjIzE4ws9+bWaWZ/TMcLohZZq2ZXRUOl5rZy2a2NJz3PTOb2sJ5h5nZOjOrMbPnzeweM3s0Sd2p\n1PhTM/tz2N4fzGxAzPQrzGyHmVWZ2e2NbJ+JZvaRmWXFjLvUzN4IhyeY2V/MbLeZ7TKzu82sR5K2\nHjazf4t5/INwmQ/NbF7cvBeb2etmtsfMPjCzxTGT14X3u81sr5mdXbdtY5Y/x8xeM7Pq8P6cVLdN\nY8zsS+Hyu81ss5lNj5l2kZltCdvcaWbfD8cPCF+f3Wb2qZmtNzPlSzvTBu86Tgb6AUOB+QSv/UPh\n4yHAfuDuRpafCLwNDAD+D/CgmVkL5n0M+BvQH1gMXNHIOlOp8V+BbwMnAj2AuoA5HbgvbH9QuL4C\nEnD3V4HPgPPj2n0sHP4cuCl8PmcDFwDfaaRuwhouDOv5GnAaEN9//xkwFzgeuBhYYGaXhNPODe+P\nd/c8d/9LXNv9gGeAZeFz+znwjJn1j3sOx2ybJmrOBp4G/hAu9z1ghZmNCGd5kKD7rjdwJvBiOP4W\noAIYCJwE3AbouiLtTIHedRwBFrn7QXff7+5V7v6ku+9z9xpgCfCVRpbf4e6/cPfPgUeAUwj+cVOe\n18yGAOOBO9z9kLu/DKxOtsIUa3zI3f/H3fcDvwWKwvGzgN+7+zp3Pwj8JNwGyfwGmA1gZr2Bi8Jx\nuPsGd/+ru9e6eznwnwnqSOSbYX1vuftnBG9gsc9vrbu/6e5H3P2NcH2ptAvBG8Df3f3XYV2/AbYB\n/xIzT7Jt05izgDzgf4ev0YvA7wm3DXAYON3M+rj7P919Y8z4U4Ch7n7Y3de7LhTV7hToXUelux+o\ne2BmuWb2n2GXxB6Cj/jHx3Y7xPmobsDd94WDec2cdxDwacw4gA+SFZxijR/FDO+LqWlQbNthoFYl\nWxfB3vhMM+sJzAQ2uvuOsI7hYXfCR2Ed/06wt96UBjUAO+Ke30QzeynsUqoGrk2x3bq2d8SN2wHk\nxzxOtm2arNndY9/8Ytv9BsGb3Q4z+5OZnR2O/xmwHfiDmb1rZgtTexqSTgr0riN+b+kWYAQw0d37\ncPQjfrJulHTYBfQzs9yYcYMbmb81Ne6KbTtcZ/9kM7v7FoLgmkrD7hYIum62AaeFddzWkhoIuo1i\nPUbwCWWwu/cF7o9pt6m92w8JuqJiDQF2plBXU+0Ojuv/rm/X3V9z9xkE3TGrCPb8cfcad7/F3U8F\npgM3m9kFraxFmkmB3nX1JuiT3h32xy5q6xWGe7xlwGIz6xHu3f1LI4u0psYngGlm9uXwAOadNP33\n/hhwA8Ebx/+Lq2MPsNfMRgILUqzht0CpmZ0evqHE19+b4BPLATObQPBGUqeSoIvo1CRtrwGGm9m/\nmll3M/sWcDpB90hrvEqwN3+rmWWb2RSC12hl+JrNMbO+7n6YYJscATCzaWb2xfBYSTXBcYfGurik\nDSjQu667gOOAT4C/Av/VTuudQ3BgsQr4N+BxgvPlE2lxje6+GbiOIKR3Af8kOGjXmLo+7Bfd/ZOY\n8d8nCNsa4BdhzanU8Gz4HF4k6I54MW6W7wB3mlkNcAfh3m647D6CYwZ/Ds8cOSuu7SpgGsGnmCrg\nVmBaXN3N5u6HCAJ8KsF2vxeY6+7bwlmuAMrDrqdrCV5PCA76Pg/sBf4C3OvuL7WmFmk+03ELySQz\nexzY5u5t/glBJOq0hy7tyszGm9kXzKxbeFrfDIK+WBFpJX1TVNrbycBTBAcoK4AF7v56ZksSiQZ1\nuYiIRIS6XEREIiJjXS4DBgzwwsLCTK1eRKRT2rBhwyfuPjDRtIwFemFhIWVlZZlavYhIp2Rm8d8Q\nrqcuFxGRiFCgi4hEhAJdRCQiFOgiIhGhQBcRiQgFuohIRCjQRUQiQtdyEWmlgwfho49g166jt08/\nhZNPhsJCGDYMBg+GHgl/VlokfRTo0moHD8Inn4AZ9O0LubnBcGf32WcNQzr+9uGHR8O7KWaQnx8E\nfOxt2LDgfvBgyM5u06cj7cQddu+GnTuDv5GdO48dvuIKuOGG9K9bgS4NuEN1NVRWBiGd6D5+XE1N\nwzaysqBPnyDc+/ZtOJzq4z59gnba4vnt2ZM8nGNve/Ycu3x2drDnfcop8MUvwuTJwfCgQcF93a1f\nv6CN8vJjb+vXw2OPwZGY3/Pp1q3xwC8oUOB3BAcOBK9rfEjHB/f+/ccu269f8Brn5wfDbSFjV1ss\nKSlxffW/7R06BFVVqQVz3X1tbeK2jjsOBg6EAQOC+9jh/uGvdVZXB7c9e44OJ3qcbB2x8vKa/4bQ\nuzf885+N71En+mfLyTk2lGNvddP69QvCt7UOH4aKisSBX14eTIsP/IKCxgO/u3bPWuzIkeDvP9ke\ndd1wVYKfGc/JCUJ60KCjgR0/PGhQMF86mNkGdy9JOK2rB7p78M918GDD24EDx46Ln3b4cPPW0xbz\nQtA1kGxvuro6+XL9+jUM5URBHTsuNzd5W83hHoRqU6Hf1LhEwRyvd+/EwRx/69u3Y3UTxQb+e+8l\nDvzYv5OsrMSBP2hQEPRmwa1bt4b3qQw3Z95ky5kFoXnkSFB37H2y4baYXtcVEr93vWvXsf/PZnDS\nSU2H9QkntO/fTqQCfds2+N3vmh++yaYdOtT8AO2IevRoPIzjx/Xr1/n36A4dCsI9NvRrauD4448G\nda9ema6ybRw61HAPPz70d+6Mxt91W+nTp/GQzs8PutY64v9IY4HeActt3JtvwsKFwXBWFvTsGdxy\nco4Ox9569QrCK9G0ZMukOi07u3nvzG01b05OsCfakfYw20OPHsEb1IABma6k/fXoAaeeGtwSOXQI\nPvgg2POM3UuN33uNH5dsOB3zZmUduwefbLgtppsFQZ6fH3TnRVGnC/RLLgm6GHr2bJuDZiJR0KMH\nfOELwU26jk4X6NnZOtovIpKIvikqIhIRCnQRkYhQoIuIRIQCXUQkIhToIiIRoUAXEYkIBbqISEQo\n0EVEIkKBLiISEQp0EZGIaDLQzWywmb1kZlvMbLOZHfM7G2Y2xcyqzWxTeLujbcoVEZFkUrmWSy1w\ni7tvNLPewAYz+6O7b4mbb727T0t/iSIikoom99DdfZe7bwyHa4CtQH5bFyYiIs3TrD50MysEioFX\nE0w+28z+28yeNbMzkiw/38zKzKyssrKy2cWKiEhyKQe6meUBTwI3unv8z+duBIa6+xjg/wKrErXh\n7svdvcTdSwYOHNjSmkVEJIGUAt3MsgnCfIW7PxU/3d33uPvecHgNkG1mXfB3ZEREMieVs1wMeBDY\n6u4/TzLPyeF8mNmEsN0Ev48tIiJtJZWzXCYBVwBvmtmmcNxtwBAAd78fmAUsMLNaYD9wuWfq16dF\nRLqoJgPd3V8GGv35YXe/G7g7XUWJiEjz6ZuiIiIRoUAXEYkIBbqISEQo0EVEIkKBLiISEQp0EZGI\nUKCLiESEAl1EJCIU6CIiEaFAFxGJCAW6iEhEKNBFRCJCgS4iEhEKdBGRiFCgi4hEhAJdRCQiFOgi\nIhGhQBcRiQgFuohIRCjQRUQiQoEuIhIRCnQRkYhQoIuIRIQCXUQkIhToIiIRoUAXEYkIBbqISEQo\n0EVEIqLJQDezwWb2kpltMbPNZnZDgnnMzJaZ2XYze8PMxrZNuSIikkz3FOapBW5x941m1hvYYGZ/\ndPctMfNMBU4LbxOB+8J7ERFpJ03uobv7LnffGA7XAFuB/LjZZgC/8sBfgePN7JS0VysiIkk1qw/d\nzAqBYuDVuEn5wAcxjys4NvQxs/lmVmZmZZWVlc2rVEREGpVyoJtZHvAkcKO772nJytx9ubuXuHvJ\nwIEDW9KEiIgkkVKgm1k2QZivcPenEsyyExgc87ggHCciIu0klbNcDHgQ2OruP08y22pgbni2y1lA\ntbvvSmOdIiLShFTOcpkEXAG8aWabwnG3AUMA3P1+YA1wEbAd2Ad8O/2liohIY5oMdHd/GbAm5nHg\nunQVJSIizadvioqIRIQCXUQkIhToIiIRoUAXEYkIBbqISEQo0EVEIkKBLiISEQp0EZGIUKCLiESE\nAl1EJCIU6CIiEaFAFxGJCAW6iEhEKNBFRCJCgS4iEhEKdBGRiFCgi4hEhAJdRCQiFOgiIhGRyo9E\ni0hEHD58mIqKCg4cOJDpUqQJOTk5FBQUkJ2dnfIyCnSRLqSiooLevXtTWFiIWaO//S4Z5O5UVVVR\nUVHBsGHDUl5OXS4iXciBAwfo37+/wryDMzP69+/f7E9SCnSRLkZh3jm05HVSoItIu6mqqqKoqIii\noiJOPvlk8vPz6x8fOnSo0WXLysq4/vrrm1zHOeeck5Za165dy7Rp09LSVntRH7qIJLViBdx+O7z/\nPgwZAkuWwJw5LW+vf//+bNq0CYDFixeTl5fH97///frptbW1dO+eOJZKSkooKSlpch2vvPJKywvs\n5LSHLiIJrVgB8+fDjh3gHtzPnx+MT6fS0lKuvfZaJk6cyK233srf/vY3zj77bIqLiznnnHN4++23\ngYZ7zIsXL2bevHlMmTKFU089lWXLltW3l5eXVz//lClTmDVrFiNHjmTOnDm4OwBr1qxh5MiRjBs3\njuuvv77JPfFPP/2USy65hNGjR3PWWWfxxhtvAPCnP/2p/hNGcXExNTU17Nq1i3PPPZeioiLOPPNM\n1q9fn94N1gjtoYtIQrffDvv2NRy3b18wvjV76YlUVFTwyiuvkJWVxZ49e1i/fj3du3fn+eef57bb\nbuPJJ588Zplt27bx0ksvUVNTw4gRI1iwYMExp/i9/vrrbN68mUGDBjFp0iT+/Oc/U1JSwjXXXMO6\ndesYNmwYs2fPbrK+RYsWUVxczKpVq3jxxReZO3cumzZtYunSpdxzzz1MmjSJvXv3kpOTw/Lly/n6\n17/O7bffzueff86++I3YhpoMdDP7JTAN+Ie7n5lg+hTgd8B74ain3P3OdBYpIu3v/febN741Lrvs\nMrKysgCorq7myiuv5O9//ztmxuHDhxMuc/HFF9OzZ0969uzJiSeeyMcff0xBQUGDeSZMmFA/rqio\niPLycvLy8jj11FPrTwecPXs2y5cvb7S+l19+uf5N5fzzz6eqqoo9e/YwadIkbr75ZubMmcPMmTMp\nKChg/PjxzJs3j8OHD3PJJZdQVFTUqm3THKl0uTwMXNjEPOvdvSi8KcxFImDIkOaNb41evXrVD//k\nJz/hvPPO46233uLpp59Oeupez54964ezsrKora1t0TytsXDhQh544AH279/PpEmT2LZtG+eeey7r\n1q0jPz+f0tJSfvWrX6V1nY1pMtDdfR3waTvUIiIdyJIlkJvbcFxubjC+LVVXV5Ofnw/Aww8/nPb2\nR4wYwbvvvkt5eTkAjz/+eJPLTJ48mRXhwYO1a9cyYMAA+vTpwzvvvMOoUaP44Q9/yPjx49m2bRs7\nduzgpJNO4uqrr+aqq65i48aNaX8OyaTroOjZZvbfZvasmZ2RbCYzm29mZWZWVllZmaZVi0hbmDMH\nli+HoUPBLLhfvjz9/efxbr31Vn70ox9RXFyc9j1qgOOOO457772XCy+8kHHjxtG7d2/69u3b6DKL\nFy9mw4YNjB49moULF/LII48AcNddd3HmmWcyevRosrOzmTp1KmvXrmXMmDEUFxfz+OOPc8MNN6T9\nOSRjdUd9G53JrBD4fZI+9D7AEXffa2YXAf/h7qc11WZJSYmXlZU1v2IRabGtW7fypS99KdNlZNze\nvXvJy8vD3bnuuus47bTTuOmmmzJd1jESvV5mtsHdE56/2eo9dHff4+57w+E1QLaZDWhtuyIibeUX\nv/gFRUVFnHHGGVRXV3PNNddkuqS0aPVpi2Z2MvCxu7uZTSB4k6hqdWUiIm3kpptu6pB75K2VymmL\nvwGmAAPMrAJYBGQDuPv9wCxggZnVAvuByz2VfhwREUmrJgPd3Rs9697d7wbuTltFIiLSIvrqv4hI\nRCjQRUQiQoEuIu3mvPPO47nnnmsw7q677mLBggVJl5kyZQp1pzhfdNFF7N69+5h5Fi9ezNKlSxtd\n96pVq9iyZUv94zvuuIPnn3++OeUn1JEus6tAF5F2M3v2bFauXNlg3MqVK1O6QBYEV0k8/vjjW7Tu\n+EC/8847+epXv9qitjoqBbqItJtZs2bxzDPP1P+YRXl5OR9++CGTJ09mwYIFlJSUcMYZZ7Bo0aKE\nyxcWFvLJJ58AsGTJEoYPH86Xv/zl+kvsQnCO+fjx4xkzZgzf+MY32LdvH6+88gqrV6/mBz/4AUVF\nRbzzzjuUlpbyxBNPAPDCCy9QXFzMqFGjmDdvHgcPHqxf36JFixg7diyjRo1i27ZtjT6/TF9mV5fP\nFemibrwRwt+aSJuiIrjrruTT+/Xrx4QJE3j22WeZMWMGK1eu5Jvf/CZmxpIlS+jXrx+ff/45F1xw\nAW+88QajR49O2M6GDRtYuXIlmzZtora2lrFjxzJu3DgAZs6cydVXXw3Aj3/8Yx588EG+973vMX36\ndKZNm8asWbMatHXgwAFKS0t54YUXGD58OHPnzuW+++7jxhtvBGDAgAFs3LiRe++9l6VLl/LAAw8k\nfX6Zvsyu9tBFpF3FdrvEdrf89re/ZezYsRQXF7N58+YG3SPx1q9fz6WXXkpubi59+vRh+vTp9dPe\neustJk+ezKhRo1ixYgWbN29utJ63336bYcOGMXz4cACuvPJK1q1bVz995syZAIwbN67+gl7JvPzy\ny1xxxRVA4svsLlu2jN27d9O9e3fGjx/PQw89xOLFi3nzzTfp3bt3o22nQnvoIl1UY3vSbWnGjBnc\ndNNNbNy4kX379jFu3Djee+89li5dymuvvcYJJ5xAaWlps3/xvk5paSmrVq1izJgxPPzww6xdu7ZV\n9dZdgrc1l99duHAhF198MWvWrGHSpEk899xz9ZfZfeaZZygtLeXmm29m7ty5rapVe+gi0q7y8vI4\n77zzmDdvXv3e+Z49e+jVqxd9+/bl448/5tlnn220jXPPPZdVq1axf/9+ampqePrpp+un1dTUcMop\np3D48OH6S94C9O7dm5qammPaGjFiBOXl5Wzfvh2AX//613zlK19p0XPL9GV2tYcuIu1u9uzZXHrp\npfVdL3WXmx05ciSDBw9m0qRJjS4/duxYvvWtbzFmzBhOPPFExo8fXz/tpz/9KRMnTmTgwIFMnDix\nPsQvv/xyrr76apYtW1Z/MBQgJyeHhx56iMsuu4za2lrGjx/Ptdde26LnVfdbp6NHjyY3N7fBZXZf\neuklunXrxhlnnMHUqVNZuXIlP/vZz8jOziYvLy8tP4SR0uVz24IunyvS/nT53M6l3S+fKyIiHYMC\nXUQkIhToIiIRoUAX6WL0cwWdQ0teJwW6SBeSk5NDVVWVQr2Dc3eqqqrIyclp1nI6bVGkCykoKKCi\nooLKyspMlyJNyMnJoaCgoFnLKNBFupDs7GyGDRuW6TKkjajLRUQkIhToIiIRoUAXEYkIBbqISEQo\n0EVEIkKBLiISEQp0EZGIUKCLiESEAl1EJCIU6CIiEdFkoJvZL83sH2b2VpLpZmbLzGy7mb1hZmPT\nX6aIiDQllT30h4ELG5k+FTgtvM0H7mt9WSIi0lxNBrq7rwM+bWSWGcCvPPBX4HgzOyVdBYqISGrS\n0YeeD3wQ87giHHcMM5tvZmVmVqbLd4qIpFe7HhR19+XuXuLuJQMHDmzPVYuIRF46An0nMDjmcUE4\nTkRE2lE6An01MDc82+UsoNrdd6WhXRERaYYmf7HIzH4DTAEGmFkFsAjIBnD3+4E1wEXAdmAf8O22\nKlZERJJrMtDdfXYT0x24Lm0ViYhIi+iboiIiEaFAFxGJCAW6iEhEKNBFRCJCgS4iEhEKdBGRiFCg\ni4hEhAJdRCQiFOgiIhGhQBcRiQgFuohIRCjQRUQiQoEuIhIRCnQRkYhQoIuIRIQCXUQkIhToIiIR\noUAXEYkIBbqISEQo0EVEIkKBLiISEQp0EZGIUKCLiESEAl1EJCIU6CIiEaFAFxGJCAW6iEhEpBTo\nZnahmb1tZtvNbGGC6aVmVmlmm8LbVekvVUREGtO9qRnMLAu4B/gaUAG8Zmar3X1L3KyPu/t326BG\nERFJQSp76BOA7e7+rrsfAlYCM9q2LBERaa5UAj0f+CDmcUU4Lt43zOwNM3vCzAYnasjM5ptZmZmV\nVVZWtqBcERFJJl0HRZ8GCt19NPBH4JFEM7n7cncvcfeSgQMHpmnVIiICqQX6TiB2j7sgHFfP3avc\n/WD48AFgXHrKExGRVKUS6K8Bp5nZMDPrAVwOrI6dwcxOiXk4HdiavhJFRCQVTZ7l4u61ZvZd4Dkg\nC/ilu282szuBMndfDVxvZtOBWuBToLQNaxYRkQTM3TOy4pKSEi8rK8vIukVEOisz2+DuJYmm6Zui\nIiIRoUAXEYkIBbqISEQo0EVEIkKBLiISEZ0q0FesgMJC6NYtuF+xItMViYh0HE2eh95RrFgB8+fD\nvn3B4x07gscAc+Zkri4RkY6i0+yh33770TCvs29fMF5ERDpRoL//fvPGi4h0NZ0m0IcMad54EZGu\nptME+pIlkJvbcFxubjBeREQ6UaDPmQPLl8PQoWAW3C9frgOiIiJ1Os1ZLhCEtwJcRCSxTrOHLiIi\njVOgi4hEhAJdRCQiFOgiIhGhQBcRiQgFuohIRCjQRUQiQoEuIhIRCvQW0HXZRaQjUqA3U9112Xfs\nAPej12XPRKh3lDcW1dHx6ugINaiODNTh7hm5jRs3zjujoUPdgyhveBs6tH3rePRR99zchjXk5gbj\nVUfXrqMj1KA62q4OoMyT5KoF09tfSUmJl5WVZWTdrdGtW/BSxDODI0far47CwuDTQbyhQ6G8XHV0\n5To6Qg2qo+3qMLMN7l6ScJoCvXk6yh9HR3ljUR0dr46OUIPqaLs6Ggt09aE3U0e5LntH+cEP1dHx\n6ugINaiOzNShQG+mjnJd9o7yxqI6Ol4dHaEG1ZGhOpJ1rsfegAuBt4HtwMIE03sCj4fTXwUKm2qz\nsx4U7UgefTQ4GGsW3Lf3QR7V0XHr6Ag1qI62qYPWHBQ1syzgf4CvARXAa8Bsd98SM893gNHufq2Z\nXQ5c6u7faqzdztqHLiKSSa3tQ58AbHf3d939ELASmBE3zwzgkXD4CeACM7OWFiwiIs2XSqDnAx/E\nPK4IxyWcx91rgWqgf3xDZjbfzMrMrKyysrJlFYuISELtelDU3Ze7e4m7lwwcOLA9Vy0iEnmpBPpO\nYHDM44JwXMJ5zKw70BeoSkeBIiKSmlQC/TXgNDMbZmY9gMuB1XHzrAauDIdnAS96U0dbRUQkrVL6\npqiZXQTcBWQBv3T3JWZ2J8HpM6vNLAf4NVAMfApc7u7vNtFmJZDgO5cpGQB80sJlo0jboyFtj6O0\nLRqKwvYY6u4J+6wz9tX/1jCzsmSn7XRF2h4NaXscpW3RUNS3h74pKiISEQp0EZGI6KyBvjzTBXQw\n2h4NaXscpW3RUKS3R6fsQxcRkWN11j10ERGJo0AXEYmIThfoZnahmb1tZtvNbGGm68kkMxtsZi+Z\n2RYz22xmN2S6pkwzsywze93Mfp/pWjLNzI43syfMbJuZbTWzszNdU6aY2U3h/8hbZvab8LszkdOp\nAj28lO89wFTgdGC2mZ2e2aoyqha4xd1PB84Cruvi2wPgBmBrpovoIP4D+C93HwmMoYtuFzPLB64H\nStz9TIIvSF6e2araRqcKdFK7lG+X4e673H1jOFxD8A8bfyXMLsPMCoCLgQcyXUummVlf4FzgQQB3\nP+TuuzNbVUZ1B44LrzWVC3yY4XraRGcL9FQu5dslmVkhwaUXXs1sJRl1F3Ar0I4//dthDQMqgYfC\nLqgHzKxXpovKBHffCSwF3gd2AdXu/ofMVtU2OlugSwJmlgc8Cdzo7nsyXU8mmNk04B/uviHTtXQQ\n3YGxwH3uXgx8BnTJY05mdgLBJ/lhwCCgl5n9r8xW1TY6W6CncinfLsXMsgnCfIW7P5XpejJoEjDd\nzMoJuuLON7NHM1tSRlUAFe5e94ntCYKA74q+Crzn7pXufhh4CjgnwzW1ic4W6KlcyrfLCH/m70Fg\nq7v/PNP1ZJK7/8jdC9y9kODv4kV3j+ReWCrc/SPgAzMbEY66ANjSyCJR9j5wlpnlhv8zFxDRA8Td\nM11Ac7h7rZl9F3iOo5fy3ZzhsjJpEnAF8KaZbQrH3ebuazJYk3Qc3wNWhDs/7wLfznA9GeHur5rZ\nE8BGgjPDXieilwDQV/9FRCKis3W5iIhIEgp0EZGIUKCLiESEAl1EJCIU6CIiEaFAFxGJCAW6iEhE\n/H+sbXlUWFIG6AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5z7yR1GZ_fhP",
        "outputId": "78aa8a8b-7c2a-4c12-8cb8-9b10dd214966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "model_train.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_embedding (Embedding)   (None, None, 150)    300000      encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "encoder_gru1 (GRU)              [(None, None, 512),  1018368     encoder_embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "encoder_gru2 (GRU)              [(None, None, 512),  1574400     encoder_gru1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_gru3 (GRU)              [(None, None, 512),  1574400     encoder_gru2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_embedding (Embedding)   (None, None, 150)    300000      decoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_gru1 (GRU)              [(None, None, 512),  1018368     decoder_embedding[0][0]          \n",
            "                                                                 encoder_gru3[0][1]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_gru2 (GRU)              [(None, None, 512),  1574400     decoder_gru1[0][0]               \n",
            "                                                                 encoder_gru3[0][1]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_gru3 (GRU)              [(None, None, 512),  1574400     decoder_gru2[0][0]               \n",
            "                                                                 encoder_gru3[0][1]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, None, 512)    0           encoder_gru3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, None, 512)    0           decoder_gru3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "attention (Attention)           ((None, None, 512),  262144      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 1024)   0           attention[0][0]                  \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output1 (Dense)         (None, None, 512)    524800      concat_layer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output (Dense)          (None, None, 2000)   1026000     decoder_output1[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 10,747,280\n",
            "Trainable params: 10,747,280\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eURvLyXKaimb",
        "colab": {}
      },
      "source": [
        "model_train.save(\"augmenattention1.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hmw2mCLAWXCg",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    model_train.load_weights(path_checkpoint)\n",
        "except Exception as error:\n",
        "    print(\"Error trying to load checkpoint.\")\n",
        "    print(error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tyj_Vb3L_fhV",
        "outputId": "2c6d2cbd-66c8-484a-d609-268fd94f20ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%%time\n",
        "result = model_train.evaluate(x_test,y_test)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 228s 5ms/sample - loss: 3.5403 - sparse_categorical_accuracy: 0.6679\n",
            "CPU times: user 3min 41s, sys: 6.01 s, total: 3min 47s\n",
            "Wall time: 3min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lX5pwmew_fhb",
        "outputId": "ec23ad49-8fa5-41f3-8827-ca69f282390e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(\"Accuracy: {0:.2%}\".format(result[1]))\n",
        "\n",
        "print(\"Loss {0:.5}\".format(result[0]))\n",
        "loss=result[0]\n",
        "print(\"loss:\",loss)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 66.79%\n",
            "Loss 3.5403\n",
            "loss: 3.5402977410316465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8A8wvDzF_fhe",
        "colab": {}
      },
      "source": [
        "def perplexity(loss):\n",
        "    \"\"\"\n",
        "    The perplexity metric. Why isn't this part of Keras yet?!\n",
        "\n",
        "    BTW doesn't really work.\n",
        "    \"\"\"\n",
        "    #cross_entropy = sparse_cross_entropy(y_true, y_pred)\n",
        "    perplexity = np.exp(loss)\n",
        "    return perplexity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sIB1uMm1_fhh",
        "outputId": "9b68fdcb-7710-428c-82a1-88c6177a1301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "perplexity=perplexity(loss=loss)\n",
        "print(\"perplexity:\",perplexity)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "perplexity: 34.47718293482799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3f_Q8iWwghNs",
        "outputId": "c8f709fe-c9ad-4bad-d9ba-df2af9bc320d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.17.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k6ARsBF_eQa5",
        "colab": {}
      },
      "source": [
        "#save model\n",
        "model_train.save(\"mymodel.h5\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FCU0JEQf_fhk"
      },
      "source": [
        "__Respone Texts__\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1B28YVUs_fhl",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "def response(input_text, true_output_text=None):\n",
        "    \"\"\" a single text-string.\"\"\"\n",
        "\n",
        "    # Convert the input-text to integer-tokens.\n",
        "    # Note the sequence of tokens has to be reversed.\n",
        "    # Padding is probably not necessary.\n",
        "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
        "                                                padding=True)\n",
        "    #print(\"input_tokens\",input_tokens)\n",
        "    \n",
        "    # Get the output of the encoder's GRU which will be\n",
        "    # used as the initial state in the decoder's GRU.\n",
        "    # This could also have been the encoder's final state\n",
        "    # but that is really only necessary if the encoder\n",
        "    # and decoder use the LSTM instead of GRU because\n",
        "    # the LSTM has two internal states.\n",
        "    initial_state,hidden_state= model_encoder.predict(input_tokens)\n",
        "    #print(\"Encoder_output:\",initial_state.shape)\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens =tokenizer_dest.max_tokens\n",
        "    #print(\"Tokens:\",max_tokens)\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens.\n",
        "    shape= (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "    #print(decoder_input_data)\n",
        "    #decoder_input_data= [0 for x in range(max_tokens)]\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "\n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0,count_tokens]=token_int\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "           'decoder_initial_state': hidden_state,\n",
        "           'decoder_input': decoder_input_data,\n",
        "           'decoder_initial1':initial_state\n",
        "        }\n",
        "\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict()\n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "        #print(\"decoder output\",decoder_output.shape)\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0,count_tokens]\n",
        "        #print(\"Output predictrd:\",token_onehot.shape)\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "        #print(\"High probability:\",token_int)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
        "        #print(\"sampled word:\",sampled_word)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens+=1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "    print(\"Output Tokens:\",output_tokens)\n",
        "   \n",
        "    \n",
        "    # Print the input-text.\n",
        "    print(\"Input text:\")\n",
        "    print(input_text)\n",
        "    print()\n",
        "\n",
        "\n",
        "    print(\"Predicted output:\")\n",
        "    print(output_text)\n",
        "    print()\n",
        "\n",
        "    \n",
        "    if true_output_text is not None:\n",
        "        print(\"True output text:\")\n",
        "        print(true_output_text)\n",
        "        print()\n",
        "\n",
        "    score = sentence_bleu([output_text], true_output_text)\n",
        "    print(\"score is:\",score)\n",
        "\n",
        "    score = sentence_bleu([output_text], true_output_text)\n",
        "    print(\"score is:\",score)\n",
        "\n",
        "    score1 = sentence_bleu([output_text], true_output_text,weights=(1, 0, 0, 0))\n",
        "    print(\"score is for 1-gram:\",score1) \n",
        "\n",
        "    score2 = sentence_bleu([output_text], true_output_text,weights=(0, 1, 0, 0))\n",
        "    print(\"score is for 2-gram:\",score2) \n",
        "\n",
        "    score3 = sentence_bleu([output_text], true_output_text,weights=(0, 0, 1, 0))\n",
        "    print(\"score is for 3-gram:\",score3) \n",
        "\n",
        "    score4 = sentence_bleu([output_text], true_output_text,weights=(0, 0, 0, 1))\n",
        "    print(\"score is for 4-gram:\",score4) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5HZ8MEo9eQa_",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "def response1(input_text, true_output_text=None):\n",
        "    \"\"\" a single text-string.\"\"\"\n",
        "\n",
        "    # Convert the input-text to integer-tokens.\n",
        "    # Note the sequence of tokens has to be reversed.\n",
        "    # Padding is probably not necessary.\n",
        "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
        "                                                padding=True)\n",
        "    #print(\"input_tokens\",input_tokens)\n",
        "    \n",
        "    # Get the output of the encoder's GRU which will be\n",
        "    # used as the initial state in the decoder's GRU.\n",
        "    # This could also have been the encoder's final state\n",
        "    # but that is really only necessary if the encoder\n",
        "    # and decoder use the LSTM instead of GRU because\n",
        "    # the LSTM has two internal states.\n",
        "    initial_state,hidden_state = model_encoder.predict(input_tokens)\n",
        "    #print(\"Encoder_output:\",initial_state.shape)\n",
        "\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens =tokenizer_dest.max_tokens\n",
        "    #print(\"Tokens:\",max_tokens)\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens.\n",
        "    shape= (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "    #print(decoder_input_data)\n",
        "    #decoder_input_data= [0 for x in range(max_tokens)]\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "\n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0,count_tokens]=token_int\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "           'decoder_initial_state': hidden_state,\n",
        "           'decoder_input': decoder_input_data,\n",
        "           'decoder_initial1':initial_state\n",
        "        }\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict()\n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "        #print(\"decoder output\",decoder_output.shape)\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0,count_tokens]\n",
        "        #print(\"Output predictrd:\",token_onehot.shape)\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "        #print(\"High probability:\",token_int)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
        "        #print(\"sampled word:\",sampled_word)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens+=1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "    #print(\"Output Tokens:\",output_tokens)\n",
        "   \n",
        "    \n",
        "    # Print the input-text.\n",
        "    #print(\"Input text:\")\n",
        "    #print(input_text)\n",
        "    #print()\n",
        "\n",
        "\n",
        "    #print(\"Predicted output:\")\n",
        "    #print(output_text)\n",
        "    #print()\n",
        "\n",
        "    \n",
        "    #if true_output_text is not None:\n",
        "     #   print(\"True output text:\")\n",
        "      #  print(true_output_text)\n",
        "       # print()\n",
        "    \n",
        "\n",
        "    #score = sentence_bleu(output_text,true_output_text, weights=(1, 0, 0, 0))\n",
        "    #print(\"Blue score:\")\n",
        "    #print(score)\n",
        "    return input_text,output_text,true_output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "37T-HKMreQbD",
        "colab": {}
      },
      "source": [
        "def BLEU():\n",
        "    scores_list = []\n",
        "    for idx in range(0,50000): # Doing for 100 lines\n",
        "        input_text, output_text, true_output_text = response1(input_text=data_src5[idx],true_output_text=data_dest3[idx],)\n",
        "        scor = sentence_bleu([output_text], true_output_text)\n",
        "        scores_list.append(scor)\n",
        "        #print(scor)\n",
        "        \n",
        "    BLEU_average = sum(scores_list)/ 50000\n",
        "    print (\"The BLEU average score for the test_data = \", BLEU_average)\n",
        "    #print(count)\n",
        "    \n",
        "    return BLEU_average"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2roAXskxeQbG",
        "colab": {}
      },
      "source": [
        "BLEU_average = BLEU()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FmNMSNxmgWlb",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "def response2(input_text, true_output_text=None):\n",
        "    \"\"\" a single text-string.\"\"\"\n",
        "\n",
        "    # Convert the input-text to integer-tokens.\n",
        "    # Note the sequence of tokens has to be reversed.\n",
        "    # Padding is probably not necessary.\n",
        "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
        "                                                padding=True)\n",
        "    #print(\"input_tokens\",input_tokens)\n",
        "    \n",
        "    # Get the output of the encoder's GRU which will be\n",
        "    # used as the initial state in the decoder's GRU.\n",
        "    # This could also have been the encoder's final state\n",
        "    # but that is really only necessary if the encoder\n",
        "    # and decoder use the LSTM instead of GRU because\n",
        "    # the LSTM has two internal states.\n",
        "    initial_state,hidden_state = model_encoder.predict(input_tokens)\n",
        "    #print(\"Encoder_output:\",initial_state.shape)\n",
        "\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens =tokenizer_dest.max_tokens\n",
        "    #print(\"Tokens:\",max_tokens)\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens.\n",
        "    shape= (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "    #print(decoder_input_data)\n",
        "    #decoder_input_data= [0 for x in range(max_tokens)]\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "\n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0,count_tokens]=token_int\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "           'decoder_initial_state': hidden_state,\n",
        "           'decoder_input': decoder_input_data,\n",
        "           'decoder_initial1':initial_state\n",
        "        }\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict()\n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "        #print(\"decoder output\",decoder_output.shape)\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0,count_tokens]\n",
        "        #print(\"Output predictrd:\",token_onehot.shape)\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "        #print(\"High probability:\",token_int)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
        "        #print(\"sampled word:\",sampled_word)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens+=1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "    #print(\"Output Tokens:\",output_tokens)\n",
        "   \n",
        "    \n",
        "    # Print the input-text.\n",
        "    #print(\"Input text:\")\n",
        "    #print(input_text)\n",
        "    #print()\n",
        "\n",
        "\n",
        "    #print(\"Predicted output:\")\n",
        "    #print(output_text)\n",
        "    #print()\n",
        "\n",
        "    \n",
        "    #if true_output_text is not None:\n",
        "     #   print(\"True output text:\")\n",
        "      #  print(true_output_text)\n",
        "       # print()\n",
        "    \n",
        "\n",
        "    #score = sentence_bleu(output_text,true_output_text, weights=(1, 0, 0, 0))\n",
        "    #print(\"Blue score:\")\n",
        "    #print(score)\n",
        "    return input_text,output_text,true_output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lmhtCqPagW4_",
        "colab": {}
      },
      "source": [
        "def BLEU1():\n",
        "    scores_list1 = []\n",
        "    for idx in range(0,50000): # Doing for 100 lines\n",
        "        input_text, output_text, true_output_text = response2(input_text=data_src5[idx],true_output_text=data_dest3[idx],)\n",
        "        scor1 = sentence_bleu([output_text], true_output_text,weights(1,0,0,0))\n",
        "        scores_list1.append(scor1)\n",
        "        #print(scor)\n",
        "        \n",
        "    BLEU_average1= sum(scores_list1)/ 50000\n",
        "    print (\"The BLEU average score for the test_data = \", BLEU_average1)\n",
        "    #print(count)\n",
        "    \n",
        "    return BLEU_average1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kRegZp9dgXGJ",
        "colab": {}
      },
      "source": [
        "BLEU_average1 = BLEU1()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I9bzAZExgXWL",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "def response3(input_text, true_output_text=None):\n",
        "    \"\"\" a single text-string.\"\"\"\n",
        "\n",
        "    # Convert the input-text to integer-tokens.\n",
        "    # Note the sequence of tokens has to be reversed.\n",
        "    # Padding is probably not necessary.\n",
        "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
        "                                                padding=True)\n",
        "    #print(\"input_tokens\",input_tokens)\n",
        "    \n",
        "    # Get the output of the encoder's GRU which will be\n",
        "    # used as the initial state in the decoder's GRU.\n",
        "    # This could also have been the encoder's final state\n",
        "    # but that is really only necessary if the encoder\n",
        "    # and decoder use the LSTM instead of GRU because\n",
        "    # the LSTM has two internal states.\n",
        "    initial_state,hidden_state = model_encoder.predict(input_tokens)\n",
        "    #print(\"Encoder_output:\",initial_state.shape)\n",
        "\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens =tokenizer_dest.max_tokens\n",
        "    #print(\"Tokens:\",max_tokens)\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens.\n",
        "    shape= (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "    #print(decoder_input_data)\n",
        "    #decoder_input_data= [0 for x in range(max_tokens)]\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "\n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0,count_tokens]=token_int\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "           'decoder_initial_state': hidden_state,\n",
        "           'decoder_input': decoder_input_data,\n",
        "           'decoder_initial1':initial_state\n",
        "        }\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict()\n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "        #print(\"decoder output\",decoder_output.shape)\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0,count_tokens]\n",
        "        #print(\"Output predictrd:\",token_onehot.shape)\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "        #print(\"High probability:\",token_int)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
        "        #print(\"sampled word:\",sampled_word)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens+=1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "    #print(\"Output Tokens:\",output_tokens)\n",
        "   \n",
        "    \n",
        "    # Print the input-text.\n",
        "    #print(\"Input text:\")\n",
        "    #print(input_text)\n",
        "    #print()\n",
        "\n",
        "\n",
        "    #print(\"Predicted output:\")\n",
        "    #print(output_text)\n",
        "    #print()\n",
        "\n",
        "    \n",
        "    #if true_output_text is not None:\n",
        "     #   print(\"True output text:\")\n",
        "      #  print(true_output_text)\n",
        "       # print()\n",
        "    \n",
        "\n",
        "    #score = sentence_bleu(output_text,true_output_text, weights=(1, 0, 0, 0))\n",
        "    #print(\"Blue score:\")\n",
        "    #print(score)\n",
        "    return input_text,output_text,true_output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Co76vp7XgXjF",
        "colab": {}
      },
      "source": [
        "def BLEU2():\n",
        "    scores_list2 = []\n",
        "    for idx in range(0,50000): # Doing for 100 lines\n",
        "        input_text, output_text, true_output_text = response3(input_text=data_src5[idx],true_output_text=data_dest3[idx],)\n",
        "        scor2 = sentence_bleu([output_text], true_output_text,weights(0,1,0,0))\n",
        "        scores_list2.append(scor2)\n",
        "        #print(scor)\n",
        "        \n",
        "    BLEU_average2 = sum(scores_list2)/ 50000\n",
        "    print (\"The BLEU average score for the test_data = \", BLEU_average2)\n",
        "    #print(count)\n",
        "    \n",
        "    return BLEU_average2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VfUsPFwbhJM5",
        "colab": {}
      },
      "source": [
        "BLEU_average2 = BLEU2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p8xF0jdAgXtv",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "def response4(input_text, true_output_text=None):\n",
        "    \"\"\" a single text-string.\"\"\"\n",
        "\n",
        "    # Convert the input-text to integer-tokens.\n",
        "    # Note the sequence of tokens has to be reversed.\n",
        "    # Padding is probably not necessary.\n",
        "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
        "                                                padding=True)\n",
        "    #print(\"input_tokens\",input_tokens)\n",
        "    \n",
        "    # Get the output of the encoder's GRU which will be\n",
        "    # used as the initial state in the decoder's GRU.\n",
        "    # This could also have been the encoder's final state\n",
        "    # but that is really only necessary if the encoder\n",
        "    # and decoder use the LSTM instead of GRU because\n",
        "    # the LSTM has two internal states.\n",
        "    initial_state,hidden_state = model_encoder.predict(input_tokens)\n",
        "    #print(\"Encoder_output:\",initial_state.shape)\n",
        "\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens =tokenizer_dest.max_tokens\n",
        "    #print(\"Tokens:\",max_tokens)\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens.\n",
        "    shape= (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "    #print(decoder_input_data)\n",
        "    #decoder_input_data= [0 for x in range(max_tokens)]\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "\n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0,count_tokens]=token_int\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "           'decoder_initial_state': hidden_state,\n",
        "           'decoder_input': decoder_input_data,\n",
        "           'decoder_initial1':initial_state\n",
        "        }\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict()\n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "        #print(\"decoder output\",decoder_output.shape)\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0,count_tokens]\n",
        "        #print(\"Output predictrd:\",token_onehot.shape)\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "        #print(\"High probability:\",token_int)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
        "        #print(\"sampled word:\",sampled_word)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens+=1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "    #print(\"Output Tokens:\",output_tokens)\n",
        "   \n",
        "    \n",
        "    # Print the input-text.\n",
        "    #print(\"Input text:\")\n",
        "    #print(input_text)\n",
        "    #print()\n",
        "\n",
        "\n",
        "    #print(\"Predicted output:\")\n",
        "    #print(output_text)\n",
        "    #print()\n",
        "\n",
        "    \n",
        "    #if true_output_text is not None:\n",
        "     #   print(\"True output text:\")\n",
        "      #  print(true_output_text)\n",
        "       # print()\n",
        "    \n",
        "\n",
        "    #score = sentence_bleu(output_text,true_output_text, weights=(1, 0, 0, 0))\n",
        "    #print(\"Blue score:\")\n",
        "    #print(score)\n",
        "    return input_text,output_text,true_output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IbN9UeoehfPO",
        "colab": {}
      },
      "source": [
        "def BLEU3():\n",
        "    scores_list3 = []\n",
        "    for idx in range(0,50000): # Doing for 100 lines\n",
        "        input_text, output_text, true_output_text = response4(input_text=data_src5[idx],true_output_text=data_dest3[idx],)\n",
        "        scor3 = sentence_bleu([output_text], true_output_text,weights(0,0,1,0))\n",
        "        scores_list3.append(scor3)\n",
        "        #print(scor)\n",
        "        \n",
        "    BLEU_average3= sum(scores_list3)/ 50000\n",
        "    print (\"The BLEU average score for the test_data = \", BLEU_average3)\n",
        "    #print(count)\n",
        "    \n",
        "    return BLEU_average3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w4DUGa3UhiJY",
        "colab": {}
      },
      "source": [
        "BLEU_average3 = BLEU3()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KDlDywvMht_B",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "def response5(input_text, true_output_text=None):\n",
        "    \"\"\" a single text-string.\"\"\"\n",
        "\n",
        "    # Convert the input-text to integer-tokens.\n",
        "    # Note the sequence of tokens has to be reversed.\n",
        "    # Padding is probably not necessary.\n",
        "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
        "                                                padding=True)\n",
        "    #print(\"input_tokens\",input_tokens)\n",
        "    \n",
        "    # Get the output of the encoder's GRU which will be\n",
        "    # used as the initial state in the decoder's GRU.\n",
        "    # This could also have been the encoder's final state\n",
        "    # but that is really only necessary if the encoder\n",
        "    # and decoder use the LSTM instead of GRU because\n",
        "    # the LSTM has two internal states.\n",
        "    initial_state,hidden_state = model_encoder.predict(input_tokens)\n",
        "    #print(\"Encoder_output:\",initial_state.shape)\n",
        "\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens =tokenizer_dest.max_tokens\n",
        "    #print(\"Tokens:\",max_tokens)\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens.\n",
        "    shape= (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "    #print(decoder_input_data)\n",
        "    #decoder_input_data= [0 for x in range(max_tokens)]\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "\n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0,count_tokens]=token_int\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "           'decoder_initial_state': hidden_state,\n",
        "           'decoder_input': decoder_input_data,\n",
        "           'decoder_initial1':initial_state\n",
        "        }\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict()\n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "        #print(\"decoder output\",decoder_output.shape)\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0,count_tokens]\n",
        "        #print(\"Output predictrd:\",token_onehot.shape)\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "        #print(\"High probability:\",token_int)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
        "        #print(\"sampled word:\",sampled_word)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens+=1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "    #print(\"Output Tokens:\",output_tokens)\n",
        "   \n",
        "    \n",
        "    # Print the input-text.\n",
        "    #print(\"Input text:\")\n",
        "    #print(input_text)\n",
        "    #print()\n",
        "\n",
        "\n",
        "    #print(\"Predicted output:\")\n",
        "    #print(output_text)\n",
        "    #print()\n",
        "\n",
        "    \n",
        "    #if true_output_text is not None:\n",
        "     #   print(\"True output text:\")\n",
        "      #  print(true_output_text)\n",
        "       # print()\n",
        "    \n",
        "\n",
        "    #score = sentence_bleu(output_text,true_output_text, weights=(1, 0, 0, 0))\n",
        "    #print(\"Blue score:\")\n",
        "    #print(score)\n",
        "    return input_text,output_text,true_output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NE4ut5HbhuND",
        "colab": {}
      },
      "source": [
        "def BLEU4():\n",
        "    scores_list4 = []\n",
        "    for idx in range(0,50000): # Doing for 100 lines\n",
        "        input_text, output_text, true_output_text = response5(input_text=data_src5[idx],true_output_text=data_dest3[idx],)\n",
        "        scor4 = sentence_bleu([output_text], true_output_text,weights(0,0,0,1))\n",
        "        scores_list4.append(scor4)\n",
        "        #print(scor)\n",
        "        \n",
        "    BLEU_average4 = sum(scores_list4)/ 50000\n",
        "    print (\"The BLEU average score for the test_data = \", BLEU_average4)\n",
        "    #print(count)\n",
        "    \n",
        "    return BLEU_average4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ab-kpbjGhuYd",
        "colab": {}
      },
      "source": [
        "BLEU_average4=BLEU4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wWAJ5WUV_fho"
      },
      "source": [
        "%%time\n",
        "result = model.evaluate(x_test_pad, y_test)\n",
        "print(\"Accuracy: {0:.2%}\".format(result[1]))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ShDO_vsU_fho",
        "colab": {}
      },
      "source": [
        "idx = 3\n",
        "response(input_text=data_src5[idx],\n",
        "          true_output_text=data_dest3[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ua__c0idMDgA",
        "colab": {}
      },
      "source": [
        "idx = 0\n",
        "response(input_text=data_src4[idx],\n",
        "          true_output_text=data_dest2[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sByIBJhsMINO",
        "colab": {}
      },
      "source": [
        "idx = 1\n",
        "response(input_text=data_src4[idx],\n",
        "          true_output_text=data_dest2[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-0X8KQoG_fhr"
      },
      "source": [
        "idx = 5\n",
        "response(input_text=data_src3[idx],\n",
        "          true_output_text=data_dest1[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Te9mi-jj_fhs"
      },
      "source": [
        "idx = 2\n",
        "response(input_text=data_src3[idx],\n",
        "          true_output_text=data_dest1[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LHsIuidH_fht"
      },
      "source": [
        "idx = 15\n",
        "response(input_text=data_src3[idx],\n",
        "          true_output_text=data_dest1[idx])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fUGT-icrPMMA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "b17d1e84-b1f2-4c65-a7c7-314a7e0c26b3"
      },
      "source": [
        "response(input_text=\"good morning\",\n",
        "          true_output_text='good morning')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1  43  43 612 192  25   5  98   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "good morning\n",
            "\n",
            "Predicted output:\n",
            " good good morning how are you 😄 eeee\n",
            "\n",
            "True output text:\n",
            "good morning\n",
            "\n",
            "score is: 0.12451447144412296\n",
            "score is: 0.12451447144412296\n",
            "score is for 1-gram: 0.12451447144412296\n",
            "score is for 2-gram: 0.12451447144412296\n",
            "score is for 3-gram: 0.12451447144412296\n",
            "score is for 4-gram: 0.12451447144412296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H24q6xXO_fhu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "25439fe0-8828-4168-cce9-2b4696cbb77d"
      },
      "source": [
        "response(input_text=\"hi\",\n",
        "          true_output_text='hi')"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 241 123 192  25   5  44   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "hi\n",
            "\n",
            "Predicted output:\n",
            " hi there how are you 😁 eeee\n",
            "\n",
            "True output text:\n",
            "hi\n",
            "\n",
            "score is: 2.2603294069810542e-06\n",
            "score is: 2.2603294069810542e-06\n",
            "score is for 1-gram: 2.2603294069810542e-06\n",
            "score is for 2-gram: 2.2603294069810542e-06\n",
            "score is for 3-gram: 2.2603294069810542e-06\n",
            "score is for 4-gram: 2.2603294069810542e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r6zM0nTIlc4Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "ae6b1382-a8c7-4f56-85cf-6cbcb1b02737"
      },
      "source": [
        "response(input_text=\"hello\",\n",
        "          true_output_text='hello')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 192   6  99 363 371   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "hello\n",
            "\n",
            "Predicted output:\n",
            " how is work going today eeee\n",
            "\n",
            "True output text:\n",
            "hello\n",
            "\n",
            "score is: 0.007243096968683323\n",
            "score is: 0.007243096968683323\n",
            "score is for 1-gram: 0.004937848229412018\n",
            "score is for 2-gram: 0.00822974704902003\n",
            "score is for 3-gram: 0.00822974704902003\n",
            "score is for 4-gram: 0.00822974704902003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h7xjDjJ7loww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "1c4e847b-ffd5-484c-9fba-bc3e402fb23d"
      },
      "source": [
        "response(input_text=\"I'm fine what about you?\",\n",
        "          true_output_text='fine')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1  43 409  87   6  25   5  98   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "I'm fine what about you?\n",
            "\n",
            "Predicted output:\n",
            " good best time is are you 😄 eeee\n",
            "\n",
            "True output text:\n",
            "fine\n",
            "\n",
            "score is: 0.000597183097782697\n",
            "score is: 0.000597183097782697\n",
            "score is for 1-gram: 0.0003550871944212745\n",
            "score is for 2-gram: 0.000710174388842549\n",
            "score is for 3-gram: 0.000710174388842549\n",
            "score is for 4-gram: 0.000710174388842549\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hrdK83bw_fhw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "8205ebd8-35dc-4d42-bb82-1283ef8015b7"
      },
      "source": [
        "response(input_text=\"I'm fine and you?\",\n",
        "          true_output_text='fine')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1  43 234  98   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "I'm fine and you?\n",
            "\n",
            "Predicted output:\n",
            " good lot 😄 eeee\n",
            "\n",
            "True output text:\n",
            "fine\n",
            "\n",
            "score is: 0.035204773658314856\n",
            "score is: 0.035204773658314856\n",
            "score is for 1-gram: 0.012446767091965986\n",
            "score is for 2-gram: 0.049787068367863944\n",
            "score is for 3-gram: 0.049787068367863944\n",
            "score is for 4-gram: 0.049787068367863944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EXgPk7rQl5Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "2a8084e7-229d-46d6-de17-39590181bc19"
      },
      "source": [
        "response(input_text=\"What are you doing now?\",\n",
        "          true_output_text='Right now I am chatting with you!')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1   47   25 1076   17   23   56   36    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "What are you doing now?\n",
            "\n",
            "Predicted output:\n",
            " not are years on my right now eeee\n",
            "\n",
            "True output text:\n",
            "Right now I am chatting with you!\n",
            "\n",
            "score is: 0.28622807326317534\n",
            "score is: 0.28622807326317534\n",
            "score is for 1-gram: 0.5704205697726258\n",
            "score is for 2-gram: 0.29412310628901017\n",
            "score is for 3-gram: 0.21252766389915573\n",
            "score is for 4-gram: 0.18823878802496652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OnFWmcUtmCvI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "e23069df-6a5d-4d85-b4d5-c1a795b42a71"
      },
      "source": [
        "response(input_text=\"what you think about robot?\",\n",
        "          true_output_text='')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1    7   55 1781    6    3  609  224  537    7   21  566  103    4\n",
            "    8   10    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "what you think about robot?\n",
            "\n",
            "Predicted output:\n",
            " i think religion is a very human thing i don't quite see the to that eeee\n",
            "\n",
            "True output text:\n",
            "\n",
            "\n",
            "score is: 0\n",
            "score is: 0\n",
            "score is for 1-gram: 0\n",
            "score is for 2-gram: 0\n",
            "score is for 3-gram: 0\n",
            "score is for 4-gram: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X7QYRAV4Q_Ug",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "f456ab22-3e00-4267-ee21-4e5b53dd25e3"
      },
      "source": [
        "response(input_text=\"Are you robot?\",\n",
        "          true_output_text=' Yes I am! 😄Are you a real human?')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1   27  753    7  245 1024   10  354    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "Are you robot?\n",
            "\n",
            "Predicted output:\n",
            " i'm confused i didn't expect that response eeee\n",
            "\n",
            "True output text:\n",
            " Yes I am! 😄Are you a real human?\n",
            "\n",
            "score is: 0.13932864463151154\n",
            "score is: 0.13932864463151154\n",
            "score is for 1-gram: 0.3654543018141017\n",
            "score is for 2-gram: 0.07934205236753525\n",
            "score is for 3-gram: 0.020475368352912318\n",
            "score is for 4-gram: 0.6347364189402819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jUwDh1X3_fhz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "1f2467ed-771c-43f3-d25e-ff73d195996b"
      },
      "source": [
        "response(input_text=\"what is your name?\",\n",
        "          true_output_text='robot')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1  23 213   6 206  29   5  19 213  14 254  58   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "what is your name?\n",
            "\n",
            "Predicted output:\n",
            " my call is rdany but you can call me dany ☺️ eeee\n",
            "\n",
            "True output text:\n",
            "robot\n",
            "\n",
            "score is: 0.00011671378668370304\n",
            "score is: 0.00011671378668370304\n",
            "score is for 1-gram: 9.872784326934365e-05\n",
            "score is for 2-gram: 0.00012340980408667956\n",
            "score is for 3-gram: 0.00012340980408667956\n",
            "score is for 4-gram: 0.00012340980408667956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t3mLfSsbm3f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "ef7a36fa-351d-4b13-82f7-d389233d2264"
      },
      "source": [
        "response(input_text=\"are you rdany?\",\n",
        "          true_output_text='robot')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1   27  753    7  245 1024   10   51    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "are you rdany?\n",
            "\n",
            "Predicted output:\n",
            " i'm confused i didn't expect that do eeee\n",
            "\n",
            "True output text:\n",
            "robot\n",
            "\n",
            "score is: 0.0005379707292175051\n",
            "score is: 0.0005379707292175051\n",
            "score is for 1-gram: 0.0003667516566777434\n",
            "score is for 2-gram: 0.0006112527611295723\n",
            "score is for 3-gram: 0.0006112527611295723\n",
            "score is for 4-gram: 0.0006112527611295723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66l09sUq_fh2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "f3b200c1-3236-43b3-fc2d-79eb52b27e1c"
      },
      "source": [
        "response(input_text=\"you are human or robot?\",\n",
        "          true_output_text='robot')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 151   4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "you are human or robot?\n",
            "\n",
            "Predicted output:\n",
            " sorry the eeee\n",
            "\n",
            "True output text:\n",
            "robot\n",
            "\n",
            "score is: 0.11911017117880102\n",
            "score is: 0.11911017117880102\n",
            "score is for 1-gram: 0.08120116994196762\n",
            "score is for 2-gram: 0.1353352832366127\n",
            "score is for 3-gram: 0.1353352832366127\n",
            "score is for 4-gram: 0.1353352832366127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "man_zxw8_fh4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "209e14eb-239e-46b2-e271-9e7edeb4dd75"
      },
      "source": [
        "response(input_text=\"you are man or woman?\",\n",
        "          true_output_text='robot')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 151   4 225  17   6   4   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "you are man or woman?\n",
            "\n",
            "Predicted output:\n",
            " sorry the really on is the eeee\n",
            "\n",
            "True output text:\n",
            "robot\n",
            "\n",
            "score is: 0.0042715185278595835\n",
            "score is: 0.0042715185278595835\n",
            "score is for 1-gram: 0.003613264754090133\n",
            "score is for 2-gram: 0.004516580942612666\n",
            "score is for 3-gram: 0.004516580942612666\n",
            "score is for 4-gram: 0.004516580942612666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LG9OETln_fh8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "493e6d09-1bdb-49aa-9d41-f03b19b50a09"
      },
      "source": [
        "response(input_text=\"you are human or woman?\",\n",
        "          true_output_text='robot')"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [ 1 27 74 44  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0]\n",
            "Input text:\n",
            "you are human or woman?\n",
            "\n",
            "Predicted output:\n",
            " i'm here 😁 eeee\n",
            "\n",
            "True output text:\n",
            "robot\n",
            "\n",
            "score is: 0.07409853791557794\n",
            "score is: 0.07409853791557794\n",
            "score is for 1-gram: 0.022160631672466777\n",
            "score is for 2-gram: 0.11080315836233387\n",
            "score is for 3-gram: 0.11080315836233387\n",
            "score is for 4-gram: 0.11080315836233387\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iqgG75MX_fh-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "918349fa-ea56-4f1a-d079-d0672e2db3db"
      },
      "source": [
        "response(input_text=\"In which year you are born\",\n",
        "          true_output_text='2020.')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1  127 1260    6   47    3 1191   16 1107    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "In which year you are born\n",
            "\n",
            "Predicted output:\n",
            " well yellow is not a common for eyes eeee\n",
            "\n",
            "True output text:\n",
            "2020.\n",
            "\n",
            "score is: 0\n",
            "score is: 0\n",
            "score is for 1-gram: 0\n",
            "score is for 2-gram: 0\n",
            "score is for 3-gram: 0\n",
            "score is for 4-gram: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "05Zg2WuOjO1y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "508f9b8c-0680-43ba-a95a-d86f346e029d"
      },
      "source": [
        "response(input_text=\"where are you from?\",\n",
        "          true_output_text='India')"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1   27   48  438  313    9    4 1025 1126    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "where are you from?\n",
            "\n",
            "Predicted output:\n",
            " i'm from argentina south of the earth xd eeee\n",
            "\n",
            "True output text:\n",
            "India\n",
            "\n",
            "score is: 0.0002597513091839204\n",
            "score is: 0.0002597513091839204\n",
            "score is for 1-gram: 0.00021972285597771405\n",
            "score is for 2-gram: 0.00027465356997214254\n",
            "score is for 3-gram: 0.00027465356997214254\n",
            "score is for 4-gram: 0.00027465356997214254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YIdiYMWCj05z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "3ce6f6d9-76e4-4da4-eb81-1ca4313bf774"
      },
      "source": [
        "response(input_text=\"why are we here ?\",\n",
        "          true_output_text='i do not know')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1   7  54   3 324 319   7  21  33   3 145  58   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "why are we here ?\n",
            "\n",
            "Predicted output:\n",
            " i was a tell text i don't have a never ☺️ eeee\n",
            "\n",
            "True output text:\n",
            "i do not know\n",
            "\n",
            "score is: 0.020208011139176477\n",
            "score is: 0.020208011139176477\n",
            "score is for 1-gram: 0.05626127126916646\n",
            "score is for 2-gram: 0.0304748552707985\n",
            "score is for 3-gram: 0.013298118663621164\n",
            "score is for 4-gram: 0.007313965264991641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e6ObtpZt_fiC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "ff927bbe-3884-4958-8e89-f4114abd7ef6"
      },
      "source": [
        "response(input_text=\" do you have ai?\",\n",
        "          true_output_text='yes')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1   27  152   27   47 1485   17  511    3  789  446    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            " do you have ai?\n",
            "\n",
            "Predicted output:\n",
            " i'm single i'm not interested on such a complex relationship eeee\n",
            "\n",
            "True output text:\n",
            "yes\n",
            "\n",
            "score is: 5.761500001737076e-10\n",
            "score is: 5.761500001737076e-10\n",
            "score is for 1-gram: 5.055040285274604e-10\n",
            "score is for 2-gram: 3.7912802139559533e-10\n",
            "score is for 3-gram: 7.582560427911907e-10\n",
            "score is for 4-gram: 7.582560427911907e-10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qZcZEEmL2ylY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "7ade7036-ab58-4ec3-a4e2-3d801016ed0a"
      },
      "source": [
        "response(input_text=\"have you use ai?\",\n",
        "          true_output_text='yes')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1  241  123  419   20   51    5   51  116    4 1729    6  292    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "have you use ai?\n",
            "\n",
            "Predicted output:\n",
            " hi there 😞 what do you do when the brave is bad eeee\n",
            "\n",
            "True output text:\n",
            "yes\n",
            "\n",
            "score is: 5.777748519419133e-08\n",
            "score is: 5.777748519419133e-08\n",
            "score is for 1-gram: 5.777748519419133e-08\n",
            "score is for 2-gram: 5.777748519419133e-08\n",
            "score is for 3-gram: 5.777748519419133e-08\n",
            "score is for 4-gram: 5.777748519419133e-08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8Y9BO7Ef3o52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "8662ec56-4c64-4253-8a2a-65c6f669179d"
      },
      "source": [
        "response(input_text=\"what is ai?\",\n",
        "          true_output_text='atificial intelligent')"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 142   6   3 635   9  38  17 146  49   6 107   8  10   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "what is ai?\n",
            "\n",
            "Predicted output:\n",
            " i've is a strong of can't on every or is need to that eeee\n",
            "\n",
            "True output text:\n",
            "atificial intelligent\n",
            "\n",
            "score is: 0.08602188729806837\n",
            "score is: 0.08602188729806837\n",
            "score is for 1-gram: 0.12474825592523814\n",
            "score is for 2-gram: 0.01637320859018751\n",
            "score is for 3-gram: 0.16373208590187507\n",
            "score is for 4-gram: 0.16373208590187507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nn0fycUF34Ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "06492997-9378-45d3-c36f-07b29047c350"
      },
      "source": [
        "response(input_text=\"what is aiml?\",\n",
        "          true_output_text='atificial intelligent markup language')"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1  128 1896 1897    9  406   48   50 1928   10 1898  540 1899  285\n",
            "  574   24  725 1900    8   33   30   20    3 1901 1782 1836   11 1902\n",
            "   11 1903   90    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "what is aiml?\n",
            "\n",
            "Predicted output:\n",
            " after receiving hundreds of messages from one fan that finally started following her data with she promised to have him what a cookie chocolate cake and spaghetti and meatballs wikipedia eeee\n",
            "\n",
            "True output text:\n",
            "atificial intelligent markup language\n",
            "\n",
            "score is: 0.005053832047522219\n",
            "score is: 0.005053832047522219\n",
            "score is for 1-gram: 0.014748879526640455\n",
            "score is for 2-gram: 0.006737142499823417\n",
            "score is for 3-gram: 0.0004331020178457912\n",
            "score is for 4-gram: 0.015158570624602689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uIT62Yjh_fiE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "40c30768-4178-44ce-97b6-d7ad8f81e57a"
      },
      "source": [
        "response(input_text=\"can you understand english??\",\n",
        "          true_output_text='yes')"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1  27   3 104  16  33   3 282   9 408   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "can you understand english??\n",
            "\n",
            "Predicted output:\n",
            " i'm a robot for have a give of information eeee\n",
            "\n",
            "True output text:\n",
            "yes\n",
            "\n",
            "score is: 2.32435499440918e-07\n",
            "score is: 2.32435499440918e-07\n",
            "score is for 1-gram: 1.0196744016727525e-07\n",
            "score is for 2-gram: 3.059023205018258e-07\n",
            "score is for 3-gram: 3.059023205018258e-07\n",
            "score is for 4-gram: 3.059023205018258e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7t4P91l0_fiH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "c50a8369-56ab-4edf-e5eb-ebdea1bd15aa"
      },
      "source": [
        "response(input_text=\"ok bye!\",\n",
        "          true_output_text='bye')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1   7 356  15   3 299 523   3 278   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "ok bye!\n",
            "\n",
            "Predicted output:\n",
            " i live in a lab inside a virtual eeee\n",
            "\n",
            "True output text:\n",
            "bye\n",
            "\n",
            "score is: 7.748332154007093e-06\n",
            "score is: 7.748332154007093e-06\n",
            "score is for 1-gram: 5.716626068448198e-06\n",
            "score is for 2-gram: 8.574939102672297e-06\n",
            "score is for 3-gram: 8.574939102672297e-06\n",
            "score is for 4-gram: 8.574939102672297e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4oUsm9wrkguL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "f8d732d0-a300-4915-e9fa-20b499f41333"
      },
      "source": [
        "response(input_text=\"see you later\",\n",
        "          true_output_text='okk')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 192 192  25   5 311   8  98   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "see you later\n",
            "\n",
            "Predicted output:\n",
            " how how are you happy to 😄 eeee\n",
            "\n",
            "True output text:\n",
            "okk\n",
            "\n",
            "score is: 4.8143725543532057e-05\n",
            "score is: 4.8143725543532057e-05\n",
            "score is for 1-gram: 2.1120235358186548e-05\n",
            "score is for 2-gram: 6.336070607455965e-05\n",
            "score is for 3-gram: 6.336070607455965e-05\n",
            "score is for 4-gram: 6.336070607455965e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zu9nNdvYlF2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "e510d6c0-cd20-41d1-a6a3-64ed0dbb3084"
      },
      "source": [
        "response(input_text=\"Hola!\",\n",
        "          true_output_text='Hello')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 192   6  99 363 371   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "Hola!\n",
            "\n",
            "Predicted output:\n",
            " how is work going today eeee\n",
            "\n",
            "True output text:\n",
            "Hello\n",
            "\n",
            "score is: 0.006544876933242644\n",
            "score is: 0.006544876933242644\n",
            "score is for 1-gram: 0.0032918988196080122\n",
            "score is for 2-gram: 0.00822974704902003\n",
            "score is for 3-gram: 0.00822974704902003\n",
            "score is for 4-gram: 0.00822974704902003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k7JKlMGg_fiK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "338f3677-9418-4769-8e50-4c26eb498e33"
      },
      "source": [
        "response(input_text=\"do you have any programming language?\",\n",
        "          true_output_text='yes')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1   61  155 1657  504  889    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "do you have any programming language?\n",
            "\n",
            "Predicted output:\n",
            " only something basic web rdanybot eeee\n",
            "\n",
            "True output text:\n",
            "yes\n",
            "\n",
            "score is: 6.14421235332821e-06\n",
            "score is: 6.14421235332821e-06\n",
            "score is for 1-gram: 6.14421235332821e-06\n",
            "score is for 2-gram: 6.14421235332821e-06\n",
            "score is for 3-gram: 6.14421235332821e-06\n",
            "score is for 4-gram: 6.14421235332821e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c5lP5YQJ_fiM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "587a6c11-6cd2-4862-9ec1-3879e246c3e1"
      },
      "source": [
        "response(input_text=\"Whats your plan for weekend?\",\n",
        "          true_output_text='i dont know')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1   7  21  33  88 406   7 126 406  10  19  47   7  21  39 134   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "Whats your plan for weekend?\n",
            "\n",
            "Predicted output:\n",
            " i don't have much messages i charge messages that can not i don't know their eeee\n",
            "\n",
            "True output text:\n",
            "i dont know\n",
            "\n",
            "score is: 0.0012795905975943208\n",
            "score is: 0.0012795905975943208\n",
            "score is for 1-gram: 0.0015733542800576339\n",
            "score is for 2-gram: 0.0014160188520518705\n",
            "score is for 3-gram: 0.001223719995600382\n",
            "score is for 4-gram: 0.000983346425036021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_e2LMY6S_fiQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "fb44e37e-3ac3-4000-cf08-250bb503d450"
      },
      "source": [
        "response(input_text=\"what do you do?\",\n",
        "          true_output_text='doctor')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1  45   8  18 185  64   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "what do you do?\n",
            "\n",
            "Predicted output:\n",
            " nice to like being yes eeee\n",
            "\n",
            "True output text:\n",
            "doctor\n",
            "\n",
            "score is: 0.01437430645724448\n",
            "score is: 0.01437430645724448\n",
            "score is for 1-gram: 0.012780766603253696\n",
            "score is for 2-gram: 0.0051123066413014786\n",
            "score is for 3-gram: 0.025561533206507392\n",
            "score is for 4-gram: 0.025561533206507392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3pI7IW3N2NqU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "d31bfc22-ac71-44b1-f72f-738c30468e62"
      },
      "source": [
        "response(input_text=\"what is your job?\",\n",
        "          true_output_text='doctor')"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1  23 809 228   6 206  29   5  19 213  14 254  58   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "what is your job?\n",
            "\n",
            "Predicted output:\n",
            " my bring up is rdany but you can call me dany ☺️ eeee\n",
            "\n",
            "True output text:\n",
            "doctor\n",
            "\n",
            "score is: 0.00032051534988339044\n",
            "score is: 0.00032051534988339044\n",
            "score is for 1-gram: 0.0002795521899187599\n",
            "score is for 2-gram: 0.00033546262790251185\n",
            "score is for 3-gram: 0.00033546262790251185\n",
            "score is for 4-gram: 0.00033546262790251185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IbF2vClx_fiS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "79eb532a-ac1b-4b42-de45-e74d24f8fc3e"
      },
      "source": [
        "response(input_text=\"how old are you?\",\n",
        "          true_output_text='18')"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 643   6 279  17  65   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "how old are you?\n",
            "\n",
            "Predicted output:\n",
            " everything is fine on this eeee\n",
            "\n",
            "True output text:\n",
            "18\n",
            "\n",
            "score is: 0\n",
            "score is: 0\n",
            "score is for 1-gram: 0\n",
            "score is for 2-gram: 0\n",
            "score is for 3-gram: 0\n",
            "score is for 4-gram: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QGG7gwea1Wal",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "298ef402-797c-472d-dc7e-818e41ef50a2"
      },
      "source": [
        "response(input_text=\"Is battery important to you?\",\n",
        "          true_output_text='yes')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 142  74  13   6   3  17  65 191  17  90   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "Is battery important to you?\n",
            "\n",
            "Predicted output:\n",
            " i've here it is a on this times on wikipedia eeee\n",
            "\n",
            "True output text:\n",
            "yes\n",
            "\n",
            "score is: 1.1933636448385765e-07\n",
            "score is: 1.1933636448385765e-07\n",
            "score is for 1-gram: 1.047036587316212e-07\n",
            "score is for 2-gram: 7.852774404871591e-08\n",
            "score is for 3-gram: 1.5705548809743181e-07\n",
            "score is for 4-gram: 1.5705548809743181e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nVAx7oYA8qP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "74e1bb7c-3228-4c13-e331-26c1f75e87cc"
      },
      "source": [
        "response(input_text=\"Do you use any programming language?\",\n",
        "          true_output_text='yes')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1   7  33 111   4 240   9 569  18   3 593   7 107  13   8  99   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "Do you use any programming language?\n",
            "\n",
            "Predicted output:\n",
            " i have an the source of power like a smart i need it to work eeee\n",
            "\n",
            "True output text:\n",
            "yes\n",
            "\n",
            "score is: 6.851616795153921e-10\n",
            "score is: 6.851616795153921e-10\n",
            "score is for 1-gram: 5.055040285274604e-10\n",
            "score is for 2-gram: 7.582560427911907e-10\n",
            "score is for 3-gram: 7.582560427911907e-10\n",
            "score is for 4-gram: 7.582560427911907e-10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WIOSoxWn9JO6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "55e6085c-287d-4155-decf-7e568ef5bb6a"
      },
      "source": [
        "response(input_text=\"do you have yourid?\",\n",
        "          true_output_text='yes')"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1    7   21   33 1781    6    3  951  343    8    8   14   14   27\n",
            "   47  276   42    5   33    3  110    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "do you have yourid?\n",
            "\n",
            "Predicted output:\n",
            " i don't have religion is a concept difficult to to me me i'm not sure if you have a robots eeee\n",
            "\n",
            "True output text:\n",
            "yes\n",
            "\n",
            "score is: 3.442477108469977e-14\n",
            "score is: 3.442477108469977e-14\n",
            "score is for 1-gram: 3.442477108469977e-14\n",
            "score is for 2-gram: 3.442477108469977e-14\n",
            "score is for 3-gram: 3.442477108469977e-14\n",
            "score is for 4-gram: 3.442477108469977e-14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uUVTZ_TD90xJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "c8177df5-ec6e-4579-c0b9-b9fcfa1a348f"
      },
      "source": [
        "response(input_text=\"Do you know about battery?\",\n",
        "          true_output_text='yes')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1  353    8   90   23  842 1532 1501    6  921    6    3 1533 1776\n",
            " 1417 1534 1318  221  228   85 1535 1536   51    5   18   13    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "Do you know about battery?\n",
            "\n",
            "Predicted output:\n",
            " according to wikipedia my little pony friendship is magic is a children's animated fantasy comedy television series up by lauren faust do you like it eeee\n",
            "\n",
            "True output text:\n",
            "yes\n",
            "\n",
            "score is: 8.327003578968e-23\n",
            "score is: 8.327003578968e-23\n",
            "score is for 1-gram: 9.902531902761869e-23\n",
            "score is for 2-gram: 4.9512659513809345e-23\n",
            "score is for 3-gram: 9.902531902761869e-23\n",
            "score is for 4-gram: 9.902531902761869e-23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tctn5l6r_lxd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "e6d020de-ab28-4e24-f39c-3c710b3dc230"
      },
      "source": [
        "response(input_text=\"who is Robot?\",\n",
        "          true_output_text='yes')"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 142 820  85   3 943   9 907   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "who is Robot?\n",
            "\n",
            "Predicted output:\n",
            " i've been by a team of scientists eeee\n",
            "\n",
            "True output text:\n",
            "yes\n",
            "\n",
            "score is: 6.14421235332821e-06\n",
            "score is: 6.14421235332821e-06\n",
            "score is for 1-gram: 6.14421235332821e-06\n",
            "score is for 2-gram: 6.14421235332821e-06\n",
            "score is for 3-gram: 6.14421235332821e-06\n",
            "score is for 4-gram: 6.14421235332821e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QXifg6omAFfh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "bd310e68-7a6c-4089-e39d-b322af59b94c"
      },
      "source": [
        "response(input_text=\"how many legs does a dog have ?\",\n",
        "          true_output_text='four')"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1  23 421   6 206  29   5  19 213  14 254   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "how many legs does a dog have ?\n",
            "\n",
            "Predicted output:\n",
            " my list is rdany but you can call me dany eeee\n",
            "\n",
            "True output text:\n",
            "four\n",
            "\n",
            "score is: 1.5164193645974578e-05\n",
            "score is: 1.5164193645974578e-05\n",
            "score is for 1-gram: 1.6084056237441872e-05\n",
            "score is for 2-gram: 7.148469438863054e-06\n",
            "score is for 3-gram: 2.1445408316589164e-05\n",
            "score is for 4-gram: 2.1445408316589164e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dwQZP2uIBun0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "8fd743a6-7282-4b5a-9402-0ecc7ce64617"
      },
      "source": [
        "response(input_text=\"r menas robot?\",\n",
        "          true_output_text='yes')"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 241 123 192  25   5 586   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "r menas robot?\n",
            "\n",
            "Predicted output:\n",
            " hi there how are you 😄😄 eeee\n",
            "\n",
            "True output text:\n",
            "yes\n",
            "\n",
            "score is: 0.00015562941157243166\n",
            "score is: 0.00015562941157243166\n",
            "score is for 1-gram: 0.00011482150397387342\n",
            "score is for 2-gram: 0.00017223225596081014\n",
            "score is for 3-gram: 0.00017223225596081014\n",
            "score is for 4-gram: 0.00017223225596081014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sZiTeOGaA6tK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "f167505b-16bf-44bb-8b7b-cbf32a48e59f"
      },
      "source": [
        "response(input_text=\"Who loves dog?\",\n",
        "          true_output_text='cat')"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1   7 356  15   3 299 523   3 278   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "Who loves dog?\n",
            "\n",
            "Predicted output:\n",
            " i live in a lab inside a virtual eeee\n",
            "\n",
            "True output text:\n",
            "cat\n",
            "\n",
            "score is: 7.748332154007093e-06\n",
            "score is: 7.748332154007093e-06\n",
            "score is for 1-gram: 5.716626068448198e-06\n",
            "score is for 2-gram: 8.574939102672297e-06\n",
            "score is for 3-gram: 8.574939102672297e-06\n",
            "score is for 4-gram: 8.574939102672297e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h-Ah-ezFCLzs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "9651c54c-16c5-4efa-9f48-790486dce797"
      },
      "source": [
        "response(input_text=\"what is earth?\",\n",
        "          true_output_text='blue')"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1  142  358 1215   13    6 1315    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "what is earth?\n",
            "\n",
            "Predicted output:\n",
            " i've get word it is 42 eeee\n",
            "\n",
            "True output text:\n",
            "blue\n",
            "\n",
            "score is: 0.0017527424730016973\n",
            "score is: 0.0017527424730016973\n",
            "score is for 1-gram: 0.0006196880441665896\n",
            "score is for 2-gram: 0.0024787521766663585\n",
            "score is for 3-gram: 0.0024787521766663585\n",
            "score is for 4-gram: 0.0024787521766663585\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ZG8vDgPCrMO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "2d975c42-8a17-497f-c7f1-f28dcfe162c8"
      },
      "source": [
        "response(input_text=\"my name is urali . what is my name ?\",\n",
        "          true_output_text='urali')"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1    6   38  314  268    4    6 1341    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "my name is urali . what is my name ?\n",
            "\n",
            "Predicted output:\n",
            " is can't hear problem the is 'rdanybot' eeee\n",
            "\n",
            "True output text:\n",
            "urali\n",
            "\n",
            "score is: 0.00031726096547295525\n",
            "score is: 0.00031726096547295525\n",
            "score is for 1-gram: 0.0002683701023220095\n",
            "score is for 2-gram: 0.00033546262790251185\n",
            "score is for 3-gram: 0.00033546262790251185\n",
            "score is for 4-gram: 0.00033546262790251185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dc0H-aQexrBH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "7018275b-3dd4-44af-a0fa-590841bddf00"
      },
      "source": [
        "response(input_text=\"are you a robot or a human ?\",\n",
        "          true_output_text='robot')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1   7  55 231   7  19 213  41 228   9   3 756   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "are you a robot or a human ?\n",
            "\n",
            "Predicted output:\n",
            " i think interesting i can call some up of a friends eeee\n",
            "\n",
            "True output text:\n",
            "robot\n",
            "\n",
            "score is: 2.8781265446329452e-05\n",
            "score is: 2.8781265446329452e-05\n",
            "score is for 1-gram: 2.4345986406722902e-05\n",
            "score is for 2-gram: 3.0432483008403625e-05\n",
            "score is for 3-gram: 3.0432483008403625e-05\n",
            "score is for 4-gram: 3.0432483008403625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PpcTnn8QGWAP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "45c9cb68-7f49-4ab9-f895-5f741907465d"
      },
      "source": [
        "response(input_text=\"are you a leader or a follower ?\",\n",
        "          true_output_text='follower')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1  64   5  19 155 228  60 114  12   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "are you a leader or a follower ?\n",
            "\n",
            "Predicted output:\n",
            " yes you can something up would your be eeee\n",
            "\n",
            "True output text:\n",
            "follower\n",
            "\n",
            "score is: 0.010338086158236693\n",
            "score is: 0.010338086158236693\n",
            "score is for 1-gram: 0.008331747403681729\n",
            "score is for 2-gram: 0.011108996538242306\n",
            "score is for 3-gram: 0.011108996538242306\n",
            "score is for 4-gram: 0.011108996538242306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zSF0iNsQGkUD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "1ff79897-b1a9-4002-f790-79e8d6944a85"
      },
      "source": [
        "response(input_text=\"are you a follower or a leader?\",\n",
        "          true_output_text='follower')"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1  64   5  19 155 228  60 114  12   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "are you a follower or a leader?\n",
            "\n",
            "Predicted output:\n",
            " yes you can something up would your be eeee\n",
            "\n",
            "True output text:\n",
            "follower\n",
            "\n",
            "score is: 0.010338086158236693\n",
            "score is: 0.010338086158236693\n",
            "score is for 1-gram: 0.008331747403681729\n",
            "score is for 2-gram: 0.011108996538242306\n",
            "score is for 3-gram: 0.011108996538242306\n",
            "score is for 4-gram: 0.011108996538242306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aulu_U-jHmJp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "cf8dc1f7-e993-4b3f-e990-e963229a3059"
      },
      "source": [
        "response(input_text=\"are you have ai?\",\n",
        "          true_output_text='yes')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1  64  27 387 385 104  11   5   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "are you have ai?\n",
            "\n",
            "Predicted output:\n",
            " yes i'm artificial intelligence robot and you eeee\n",
            "\n",
            "True output text:\n",
            "yes\n",
            "\n",
            "score is: 1.1253517471925912e-07\n",
            "score is: 1.1253517471925912e-07\n",
            "score is for 1-gram: 1.1253517471925912e-07\n",
            "score is for 2-gram: 1.1253517471925912e-07\n",
            "score is for 3-gram: 1.1253517471925912e-07\n",
            "score is for 4-gram: 1.1253517471925912e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YJ1dSbOsJtqQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "e2c17965-ad13-4dd5-f4a2-4d875233051b"
      },
      "source": [
        "response(input_text=\"what is the symbol for power in math?\",\n",
        "          true_output_text='^')"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Input text:\n",
            "what is the symbol for power in math?\n",
            "\n",
            "Predicted output:\n",
            " is eeee\n",
            "\n",
            "True output text:\n",
            "^\n",
            "\n",
            "score is: 0\n",
            "score is: 0\n",
            "score is for 1-gram: 0\n",
            "score is for 2-gram: 0\n",
            "score is for 3-gram: 0\n",
            "score is for 4-gram: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "guB-C8pePtL_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "cbb20534-19f0-4c04-e62b-e60aee94d1f6"
      },
      "source": [
        "response(input_text=\"2+2?\",\n",
        "          true_output_text='4')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1  868   14   33    3  440   15    3 1032  868 1032    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "2+2?\n",
            "\n",
            "Predicted output:\n",
            " es me have a find in a en es en eeee\n",
            "\n",
            "True output text:\n",
            "4\n",
            "\n",
            "score is: 0\n",
            "score is: 0\n",
            "score is for 1-gram: 0\n",
            "score is for 2-gram: 0\n",
            "score is for 3-gram: 0\n",
            "score is for 4-gram: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I4iX_XGaUUH2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "6e32e2dd-5d65-4782-e063-bc73fa4ff70c"
      },
      "source": [
        "response(input_text=\"Who is prime minister of india?\",\n",
        "          true_output_text='naredra modei')"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 142 820  85   3 943   9 907   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Input text:\n",
            "Who is prime minister of india?\n",
            "\n",
            "Predicted output:\n",
            " i've been by a team of scientists eeee\n",
            "\n",
            "True output text:\n",
            "naredra modei\n",
            "\n",
            "score is: 0.06632702436805449\n",
            "score is: 0.06632702436805449\n",
            "score is for 1-gram: 0.09369365762534726\n",
            "score is for 2-gram: 0.011277940269717724\n",
            "score is for 3-gram: 0.1353352832366127\n",
            "score is for 4-gram: 0.1353352832366127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q02Qa_qY1se-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "64e89014-8ff9-4f2d-dd34-cd2aee330ff2"
      },
      "source": [
        "response(input_text=\"What is computer science?\",\n",
        "          true_output_text='4')"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [  1 353   8  90 285   6   3 451 269 525 136 275 505 105 262 291  11 309\n",
            "  85 452 453  13 454 455 439 456 510   4 105 457 458 459 460 461   3 121\n",
            " 232 462   3 446  24 524 463 464 111 270 109 450 430   3 512 203]\n",
            "Input text:\n",
            "What is computer science?\n",
            "\n",
            "Predicted output:\n",
            " according to wikipedia her is a 2013 american romantic science fiction drama film written directed and produced by spike jonze it marks jonze's solo screenwriting debut the film follows theodore twombly joaquin phoenix a man who develops a relationship with samantha scarlett johansson an intelligent computer operating system a personified through a\n",
            "\n",
            "True output text:\n",
            "4\n",
            "\n",
            "score is: 0\n",
            "score is: 0\n",
            "score is for 1-gram: 0\n",
            "score is for 2-gram: 0\n",
            "score is for 3-gram: 0\n",
            "score is for 4-gram: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U79flzE3V_FA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "9e275ecd-f4d0-4d86-8cd7-b8969de21af5"
      },
      "source": [
        "response(input_text=\"Where are you now?\",\n",
        "          true_output_text='4')"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Tokens: [   1   27   48  438  313    9    4  224  799 1126    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "Input text:\n",
            "Where are you now?\n",
            "\n",
            "Predicted output:\n",
            " i'm from argentina south of the human race xd eeee\n",
            "\n",
            "True output text:\n",
            "4\n",
            "\n",
            "score is: 0\n",
            "score is: 0\n",
            "score is for 1-gram: 0\n",
            "score is for 2-gram: 0\n",
            "score is for 3-gram: 0\n",
            "score is for 4-gram: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cbND6Uh9_fiV"
      },
      "source": [
        "In this implementation remaining the validation and testing part.\n",
        "about early stopping,callbacks and checkpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3J8wRVEs_fiW"
      },
      "source": [
        "In seq2seq model decoder using a greedy decoding.\n",
        "\n",
        "We saw how to generate (or “decode”) the response sentence by\n",
        "taking argmax on each step of the decoder.\n",
        "\n",
        "This is greedy decoding (take most probable word on each step)\n",
        "\n",
        "Problems with this method?\n",
        "\n",
        "Greedy decoding has no way to undo decisions!\n",
        "• Input: how are you?\n",
        "(i am fine thanks for asking me)\n",
        "•  i  ____\n",
        "• i am ____\n",
        "• i am thanks ____\n",
        "(no going back now...)\n",
        "\n",
        "This problem solve using two solution\n",
        "\n",
        "1)Exhaustive search decoding\n",
        "2)Beam search decoding\n",
        "\n",
        "Sequence-to-sequence: the bottleneck problem\n",
        "\n",
        "Encoding of the input sentence this needs to capture all information about the input sentence.\n",
        "Information bottleneck!\n",
        "\n",
        "Attention\n",
        "\n",
        "Attention provides a solution to the bottleneck problem.\n",
        "Core idea: on each step of the decoder, use direct connection to the encoder to focus on a particular part of the source sequence.\n",
        "\n",
        "Attention equation:-\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kGuQ22Bf_fiX",
        "colab": {}
      },
      "source": [
        "layer_embedding = model_train.get_layer('encoder_embedding')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8ZbMREc_fia",
        "colab": {}
      },
      "source": [
        "weights_embedding = layer_embedding.get_weights()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZH39amGY_fid",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f260df5c-75bf-41b0-ca44-17f6d3ef98d8"
      },
      "source": [
        "weights_embedding.shape"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 150)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-qWm-m9t_fif",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7089daad-b8bf-4075-ead1-d5851c3ded9b"
      },
      "source": [
        "token_good = tokenizer_src.word_index['good']\n",
        "token_good"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3phTM4f_fii",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8abbb1f9-6e2b-4181-beef-6ec2735124bd"
      },
      "source": [
        "token_great = tokenizer_src.word_index['great']\n",
        "token_great"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kvf3SJZE_fil",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "fe8d4394-0178-4bea-bb24-b0846d5d6114"
      },
      "source": [
        "weights_embedding[token_good]"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7.56689906e-02, -6.51542842e-02,  6.95609767e-03,  2.56554574e-01,\n",
              "       -7.21162260e-02,  1.83029212e-02,  1.01215355e-01,  1.81641169e-02,\n",
              "       -2.39405297e-02, -1.00388639e-02, -2.19922468e-01,  1.27309278e-01,\n",
              "       -8.59947354e-02, -8.87661800e-02, -7.69442841e-02, -1.97592363e-01,\n",
              "       -9.51121673e-02, -2.23083839e-01,  7.38079026e-02,  9.66070518e-02,\n",
              "       -4.17547002e-02, -1.24392457e-01, -8.91871825e-02, -3.79741341e-01,\n",
              "        9.83423963e-02, -2.31031757e-02, -1.04792513e-01,  8.34389925e-02,\n",
              "        1.13739207e-01, -8.21297467e-02, -3.72831114e-02, -5.93961217e-03,\n",
              "        5.06240502e-03, -1.13033829e-02,  8.68322030e-02,  6.36753887e-02,\n",
              "       -4.24321219e-02,  1.95348449e-02,  3.19745839e-02,  1.27599984e-01,\n",
              "        2.12800074e-02, -3.83470245e-02,  1.03905730e-01, -2.36106180e-02,\n",
              "       -1.33247539e-01, -2.23606289e-03, -1.10496096e-01, -1.05099142e-01,\n",
              "        4.25562114e-02,  8.21141005e-02,  1.36722878e-01, -1.32237105e-02,\n",
              "        6.56932145e-02,  4.17476632e-02,  2.56080413e-03, -1.19198628e-01,\n",
              "        4.20305282e-02,  2.72143669e-02,  5.67323864e-02, -2.60314588e-02,\n",
              "        5.08123524e-02, -1.15925163e-01, -3.36804867e-01,  5.42738289e-02,\n",
              "        7.59083033e-02,  8.36105496e-02, -2.03524128e-01,  6.42256346e-03,\n",
              "        6.06525578e-02, -1.54164154e-02,  5.12357615e-02, -1.36687130e-01,\n",
              "       -1.86103284e-01,  1.13202883e-02, -4.72001769e-02, -5.45901246e-02,\n",
              "       -8.21179226e-02,  3.78637924e-03,  1.10376842e-01,  2.73139831e-02,\n",
              "       -4.75842971e-05, -1.03333183e-02, -2.64632612e-01,  7.23007042e-03,\n",
              "       -2.38044634e-02, -3.60109136e-02, -8.98915753e-02,  1.66234188e-02,\n",
              "       -2.90895160e-02,  4.28090543e-02, -2.82141596e-01, -1.07196048e-01,\n",
              "        2.35337049e-01,  6.57406747e-02,  3.82034741e-02,  3.23684998e-02,\n",
              "        3.49267162e-02, -2.37836540e-02, -7.59140775e-02,  1.34690274e-02,\n",
              "       -1.41829938e-01, -1.06318615e-01, -1.42709967e-02,  8.92440826e-02,\n",
              "       -6.26306534e-02, -7.91376382e-02,  1.17147207e-01,  7.96115175e-02,\n",
              "        4.77188826e-02, -6.72317445e-02,  4.25991341e-02, -6.15054928e-02,\n",
              "       -1.74003933e-02,  8.12963918e-02,  1.33763370e-03,  1.40974045e-01,\n",
              "       -1.10518284e-01,  7.97975436e-02,  1.33228779e-01,  8.01319554e-02,\n",
              "        7.15666115e-02,  8.10805187e-02,  5.36182672e-02, -3.55093367e-03,\n",
              "       -2.19460398e-01,  3.60337794e-02, -8.75728130e-02,  8.32653940e-02,\n",
              "       -4.29955535e-02, -6.72149584e-02,  3.54788043e-02,  1.86532840e-01,\n",
              "        4.31681704e-03, -1.48469985e-01, -1.44827649e-01,  8.18321630e-02,\n",
              "       -3.41981761e-02, -1.80220723e-01,  1.07841887e-01,  2.51141816e-01,\n",
              "        4.58946861e-02, -1.43116981e-01,  1.54449278e-02,  3.21021020e-01,\n",
              "       -5.37491478e-02, -6.39383197e-02,  1.17171191e-01,  3.50820124e-02,\n",
              "       -5.79217151e-02, -5.17130271e-02], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2hD5uEG4_fio",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "a85e47f5-f7c1-4250-9a0c-f9f315c7f2e2"
      },
      "source": [
        "weights_embedding[token_great]"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.07578742,  0.01176119,  0.11358932,  0.01712054,  0.05182497,\n",
              "        0.00347148,  0.04304645, -0.04253399, -0.07424141,  0.00807731,\n",
              "        0.09989104, -0.02732461, -0.06856582,  0.06360072, -0.03714582,\n",
              "        0.28810483,  0.07276917,  0.14651027, -0.10270957, -0.07162957,\n",
              "        0.01089239, -0.0353056 ,  0.04685178, -0.05045651,  0.06809552,\n",
              "        0.02195073, -0.06000932, -0.08645814, -0.01261813,  0.0420564 ,\n",
              "        0.02010332,  0.05471443,  0.05104563,  0.07002567,  0.03292267,\n",
              "       -0.09466662,  0.07403801,  0.00033653,  0.15047945, -0.09372725,\n",
              "       -0.0301458 , -0.1297962 , -0.05512019,  0.17926025, -0.03824006,\n",
              "        0.0453494 , -0.02434653,  0.02812867,  0.05644025,  0.0351109 ,\n",
              "        0.07070775, -0.17361735, -0.14843684, -0.16698824,  0.01970792,\n",
              "        0.08433858,  0.01681574, -0.03566413,  0.06628896,  0.0727547 ,\n",
              "        0.22072786, -0.03310934,  0.04316079,  0.1374786 ,  0.17091762,\n",
              "       -0.03271817,  0.03091525,  0.11050592, -0.04451215, -0.15978779,\n",
              "       -0.09362192,  0.20058593, -0.05064249,  0.09136259,  0.05265597,\n",
              "        0.01541501, -0.18165188, -0.08001389,  0.11033796, -0.07283138,\n",
              "        0.14503513,  0.00447872, -0.01489963, -0.07868697, -0.02469024,\n",
              "        0.18147555,  0.13513717,  0.07311844, -0.06326821,  0.03718654,\n",
              "        0.1274677 ,  0.06456926, -0.10697388, -0.20488267,  0.08732891,\n",
              "       -0.0319716 , -0.13235307,  0.00827663,  0.01313707,  0.0421463 ,\n",
              "        0.0979081 , -0.0073806 , -0.15829338, -0.06780964,  0.11827628,\n",
              "        0.07925986, -0.14329548, -0.28848237,  0.05832723,  0.0417382 ,\n",
              "       -0.02361081,  0.04612568,  0.14123315,  0.09089402, -0.04485013,\n",
              "        0.09983341, -0.02484592, -0.05708819, -0.0495926 ,  0.06060986,\n",
              "       -0.01493633, -0.23336162,  0.04076945,  0.02021444, -0.025135  ,\n",
              "        0.11299116,  0.13477188, -0.13763802,  0.0958756 , -0.00604624,\n",
              "       -0.00568942,  0.04079509, -0.07487581,  0.06298565,  0.04477679,\n",
              "        0.01426817,  0.05977079,  0.10668731, -0.07599617, -0.20689434,\n",
              "       -0.07805121,  0.11654425, -0.03799277,  0.02016535,  0.11639442,\n",
              "       -0.08069836, -0.0791795 ,  0.07323541,  0.00229062, -0.05576564],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1m9KDIxr_fir",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "def print_sorted_words(word, metric='cosine'):\n",
        "    \"\"\"\n",
        "    Print the words in the vocabulary sorted according to their\n",
        "    embedding-distance to the given word.\n",
        "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the token (i.e. integer ID) for the given word.\n",
        "    token = tokenizer_src.word_index[word]\n",
        "\n",
        "    # Get the embedding for the given word. Note that the\n",
        "    # embedding-weight-matrix is indexed by the word-tokens\n",
        "    # which are integer IDs.\n",
        "    embedding = weights_embedding[token]\n",
        "\n",
        "    # Calculate the distance between the embeddings for\n",
        "    # this word and all other words in the vocabulary.\n",
        "    distances = cdist(weights_embedding, [embedding],metric=metric).T[0]\n",
        "    \n",
        "    # Get an index sorted according to the embedding-distances.\n",
        "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
        "    sorted_index = np.argsort(distances)\n",
        "    \n",
        "    # Sort the embedding-distances.\n",
        "    sorted_distances = distances[sorted_index]\n",
        "    \n",
        "    # Sort all the words in the vocabulary according to their\n",
        "    # embedding-distance. This is a bit excessive because we\n",
        "    # will only print the top and bottom words.\n",
        "    sorted_words = [ tokenizer_src.index_to_word[token] for token in sorted_index\n",
        "                    if token != 0]\n",
        "\n",
        "    # Helper-function for printing words and embedding-distances.\n",
        "    def _print_words(words, distances):\n",
        "        for word, distance in zip(words, distances):\n",
        "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
        "\n",
        "    # Number of words to print from the top and bottom of the list.\n",
        "    k = 10\n",
        "\n",
        "    print(\"Distance from '{0}':\".format(word))\n",
        "\n",
        "    # Print the words with smallest embedding-distance.\n",
        "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
        "\n",
        "    print(\"...\")\n",
        "\n",
        "    # Print the words with highest embedding-distance.\n",
        "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "89rBnwNe_fiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "e9b335b0-e12f-4782-fc1c-22ff1b81e30c"
      },
      "source": [
        "print_sorted_words('great', metric='cosine')"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distance from 'great':\n",
            "0.000 - great\n",
            "0.456 - project\n",
            "0.467 - shut\n",
            "0.491 - feed\n",
            "0.525 - motion\n",
            "0.530 - flow\n",
            "0.537 - reading\n",
            "0.537 - down😅😂😂\n",
            "0.539 - usually\n",
            "0.543 - phone\n",
            "...\n",
            "1.447 - tomorrow\n",
            "1.456 - two\n",
            "1.470 - beautiful\n",
            "1.472 - debug\n",
            "1.475 - ypu\n",
            "1.475 - 😞\n",
            "1.486 - currently\n",
            "1.488 - westworld\n",
            "1.523 - 😂😂😂😂\n",
            "1.526 - 😣\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_JRRRDo_fiw",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wjQ4t-xf_fiz",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}